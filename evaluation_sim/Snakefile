"""
    snakemake --keep-going -j 999999 --cluster "sbatch --exclude={cluster.exclude} --mem {cluster.mem} -c {cluster.cpus-per-task} -N {cluster.Nodes}  -t {cluster.runtime} -J {cluster.jobname} --mail-type={cluster.mail_type} --mail-user={cluster.mail}" --cluster-config cluster.json --configfile experiments.json --latency-wait 100 --verbose -n

    
    # Simulated

    1. Simulate reads 

    # running isONclust/isONclust2
    1. going from original reads to clusters
    2. from cluster file to fastq files


    ### Running isONcorrect
    1. From cluster fasta to corrected reads
    2. Merge all corrected read clusters

    ### Run evaluation looking for read error rate againse reference (and eventually splice site classification)

    # SIMULATED

    ### simulation evalautions
    4. Basing exon correction plot with error rate

    5. Join everything to table


    # target rules:

"""

shell.prefix("set -o pipefail; ")
# configfile: "experiments.json"

wildcard_constraints:
    nr_reads="[\d]+",

####################################################
########## standard python functions ###############
####################################################

import re
import os
import errno
import shutil
import glob

def mkdir_p(path):
    print("creating", path)
    try:
        os.makedirs(path)
    except OSError as exc:  # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise

rule all:
   input: 
          config["ROOT_OUT"] + "/evaluation_simulated/chr6/3500000/uneven/abundance.csv",
          config["ROOT_OUT"] + "/evaluation_simulated/chr6/3500000/uneven/results.csv",
          config["ROOT_OUT"] + "/evaluation_simulated/chr6/1/uneven/abundance.csv",
          config["ROOT_OUT"] + "/evaluation_simulated/chr6/1/uneven/results.csv",
          #   config["ROOT_OUT"] + "/evaluation_simulated/chr6/1000000/uniform/abundance.csv",
          # config["ROOT_OUT"] + "/evaluation_simulated/chr6/1000000/uniform/results.csv",
          #config["ROOT_OUT"], "eval_sim_table.csv"),
          #config["ROOT_OUT"], "controlled.csv")
          config["ROOT_OUT"] + "/evaluation_simulated/plots.tar.gz"

# rule simulation:
#     input: config["ROOT_OUT"] + "/eval_sim_table.csv"

rule remove_redundant:
    input: database = config["ROOT_IN"] + "/test_data/chr6_ensemble.fa",
    output: non_redundant_database = config["ROOT_IN"] + "/test_data/chr6_nonredundant.fa"
    run:
        experiment_dir = config["ROOT_IN"] + "/scripts/full_transcriptome_exp"
        shell("python {experiment_dir}/remove_redundant.py {input.database} {output.non_redundant_database}")


rule simulate:
    input:  database_filtered = rules.remove_redundant.output.non_redundant_database
    output: reads =  config["ROOT_OUT"] + "/data/{dataset}/{nr_reads}/{abundance}/reads.fq"
    run:
        experiment_dir = config["ROOT_IN"] + "/scripts/full_transcriptome_exp"
        mkdir_p(config["ROOT_OUT"] + "/data/{0}/{1}/{2}".format(wildcards.dataset, wildcards.nr_reads, wildcards.abundance) )
        if wildcards.nr_reads == "1":
            shell("python {experiment_dir}/simulate_reads.py {input.database_filtered} {output.reads} {wildcards.nr_reads} --uneven --subsampling_experiment")            
        elif wildcards.abundance == "uneven":
            shell("python {experiment_dir}/simulate_reads.py {input.database_filtered} {output.reads} {wildcards.nr_reads} --uneven")
        else:
            shell("python {experiment_dir}/simulate_reads.py {input.database_filtered} {output.reads} {wildcards.nr_reads}")

rule isONclust:
    input:  fastq = rules.simulate.output.reads,
    output: time_and_mem =  config["ROOT_OUT"] + "/time_and_mem/{dataset}/{nr_reads}/{abundance}/time_and_mem.txt",
            clusters = config["ROOT_OUT"] + "/clustering/{dataset}/{nr_reads}/{abundance}/final_clusters.tsv" 
    run:
        time = config["GNUTIME"]
        mkdir_p(config["ROOT_OUT"] + "/time_and_mem/{0}/{1}/{2}/".format(wildcards.dataset, wildcards.nr_reads,  wildcards.abundance) )
        outfolder = config["ROOT_OUT"] + "/clustering/{0}/{1}/{2}/".format(wildcards.dataset, wildcards.nr_reads,  wildcards.abundance)
        mkdir_p(outfolder)
        shell("{time} python /galaxy/home/ksahlin/prefix/source/isONclust/isONclust --t 50 --k 13 --w 20  --q 4.0 --fastq {input.fastq}  --outfolder {outfolder}  2>&1 | tee {output.time_and_mem}")
    

rule clusters_to_fastq:
    input: fastq = rules.isONclust.input.fastq,
            clusters = rules.isONclust.output.clusters
    output: flag = config["ROOT_OUT"] + "/clustering/{dataset}/{nr_reads}/{abundance}/rule_complete.txt"  
    run:
        time = config["GNUTIME"]
        outfolder = config["ROOT_OUT"] + "/clustering/{0}/{1}/{2}/fastq/".format(wildcards.dataset, wildcards.nr_reads, wildcards.abundance)
        shell("{time} python /galaxy/home/ksahlin/prefix/source/isONclust/isONclust write_fastq --clusters {input.clusters} --fastq {input.fastq} --outfolder {outfolder} --N 1")
        shell("touch {output.flag}")


rule isoncorrect:
    input:  rules.clusters_to_fastq.output.flag
    output:  flag = config["ROOT_OUT"] + "/correction/{dataset}/{nr_reads}/{abundance}/rule_complete.txt" 
    run: 
        time = config["GNUTIME"]
        infolder =  config["ROOT_OUT"] + "/clustering/{0}/{1}/{2}/fastq/".format(wildcards.dataset, wildcards.nr_reads, wildcards.abundance) 
        outfolder = config["ROOT_OUT"] + "/correction/{0}/{1}/{2}/".format(wildcards.dataset, wildcards.nr_reads, wildcards.abundance)   
        isoncorrect_dir = config["ROOT_IN"]
        time_and_mem = config["ROOT_OUT"] + "/time_and_mem/{0}/{1}/{2}/correction_time_and_mem.txt".format(wildcards.dataset, wildcards.nr_reads, wildcards.abundance)
        shell("{time} python {isoncorrect_dir}/run_isoncorrect  --fastq_folder {infolder}  --outfolder {outfolder} --set_w_dynamically --t 62 2>&1 | tee {time_and_mem}")
        shell("touch {output.flag}")


rule combine_isoncorrect:
    input: rules.isoncorrect.output.flag
    output: corrected_reads_fastq =  config["ROOT_OUT"] + "/correction/{dataset}/{nr_reads}/{abundance}/isONcorrect.fq"
    run:
        # all_clusters_fastq = expand('/nfs/brubeck.bx.psu.edu/scratch4/ksahlin/isoncorrect/mouse_ont_min_phred_q6/fastq_clusters/{clusterid}/corrected_reads.fastq', clusterid=[str(i) for i in range(0,62747)])
        shell("> {output.corrected_reads_fastq}")
        for f in glob.glob(  config["ROOT_OUT"] + '/correction/{0}/{1}/{2}/*/corrected_reads.fastq'.format(wildcards.dataset, wildcards.nr_reads, wildcards.abundance)):
            shell('cat {f} >> {output.corrected_reads_fastq}')


rule error_rate_analysis:
    input: original_reads = rules.simulate.output.reads,
            corrected_reads = rules.combine_isoncorrect.output.corrected_reads_fastq,
            database_filtered = rules.remove_redundant.output.non_redundant_database
    output: error_rates_file = config["ROOT_OUT"] + "/evaluation_simulated/{dataset}/{nr_reads}/{abundance}/results.csv"
    run:
        experiment_dir = config["ROOT_IN"] + "/scripts/full_transcriptome_exp"
        shell("python {experiment_dir}/get_error_rates.py  {input.database_filtered} {input.original_reads}  {input.corrected_reads}  > {output.error_rates_file}")
        # error_plot=$outbase/$id/$depth/"error_rates_"$depth".pdf" 
        # shell("python {experiment_dir}/plot_error_rates.py $error_rates_file  $error_plot")


rule align_corrected_reads_minimap2:
    input: corrected_reads = rules.combine_isoncorrect.output.corrected_reads_fastq,
            ref = rules.remove_redundant.output.non_redundant_database
    output: corrected_reads_aligned = config["ROOT_OUT"] + "/correction/{dataset}/{nr_reads}/{abundance}/isONcorrect.sam"
    run:
        shell("/usr/bin/time -v  minimap2 --eqx -a -t 8  -k13 -w4 {input.ref} {input.corrected_reads} >  {output.corrected_reads_aligned} ")


rule align_original_reads_minimap2:
    input:  original_reads = rules.simulate.output.reads,
            ref = rules.remove_redundant.output.non_redundant_database
    output: original_reads_aligned =  config["ROOT_OUT"] + "/data/{dataset}/{nr_reads}/{abundance}/reads.sam"
    run:
        shell("/usr/bin/time -v  minimap2 --eqx -a -t 8  -k13 -w4 {input.ref} {input.original_reads} >  {output.original_reads_aligned} ")


rule overcorrection_analysis:
    input: original_reads_alignments = rules.align_original_reads_minimap2.output.original_reads_aligned,
            corrected_reads_alignments = rules.align_corrected_reads_minimap2.output.corrected_reads_aligned,
            original_reads = rules.simulate.output.reads,
            corrected_reads = rules.combine_isoncorrect.output.corrected_reads_fastq,
            ref = rules.remove_redundant.output.non_redundant_database
    output: abundance_file = config["ROOT_OUT"] + "/evaluation_simulated/{dataset}/{nr_reads}/{abundance}/abundance.csv"
    run:
        experiment_dir = config["ROOT_IN"] + "/scripts/full_transcriptome_exp"
        shell('echo -n  "read_acc","aligned_to","transcript_abundance","is_tp","read_type","ed_btw_transcripts","ed_read_to_true","ed_read_to_aligned"$"\n" > {output.abundance_file}')
        # shell('echo -n  "id","cov_aln","cov_true","seq","type"$"\n" > {output.abundance_file}')
        shell("python  {experiment_dir}/get_abundance.py {input.corrected_reads_alignments} {input.ref} {input.corrected_reads}   'corrected' >> {output.abundance_file}")
        shell("python  {experiment_dir}/get_abundance.py {input.original_reads_alignments} {input.ref} {input.original_reads}   'original' >>  {output.abundance_file}")
        # abundance_plot=$outbase/$id/$depth/"abundance.pdf" 
        # shell("python $experiment_dir/plot_abundance.py {output.abundance_file} 'transcript' $abundance_plot")



rule plots_full:
    input: sim_csv = config["ROOT_OUT"] + "/evaluation_simulated/chr6/3500000/uneven/results.csv"
    output: plot = config["ROOT_OUT"] + "/evaluation_simulated/sim_full.pdf"
    run:
        eval_dir = config["ROOT_IN"] + "/evaluation_sim/"
        outfolder = config["ROOT_OUT"] + "/evaluation_simulated/"          
        shell("python {eval_dir}/plots.py  {input.sim_csv}  {outfolder} sim")


rule plots_subsampling:
    input: abundance_file = config["ROOT_OUT"] + "/evaluation_simulated/chr6/1/uneven/abundance.csv",
           error_rates_file = config["ROOT_OUT"] + "/evaluation_simulated/chr6/1/uneven/results.csv"
    output: error_rate_plot = config["ROOT_OUT"] + "/evaluation_simulated/sim_subsample_error_rate.pdf",
            abundance_plot = config["ROOT_OUT"] + "/evaluation_simulated/sim_subsample_abundance.pdf"
    run:
        eval_dir = config["ROOT_IN"] + "/evaluation_sim/"
        shell("python {eval_dir}/plot_error_rates.py  {input.error_rates_file}  {output.error_rate_plot} ")
        shell("python {eval_dir}/plot_abundance_diff.py  {input.abundance_file}  {output.abundance_plot} ")
        # python full_transcriptome_exp/plot_error_rates.py  /Users/kxs624/Documents/workspace/isONcorrect/data/results/sim_exponential_results.csv  ~/tmp/ISONCORRECT/RESULTS_02_12_19/sim/err_rate 
        # python full_transcriptome_exp/plot_abundance_diff.py    /Users/kxs624/Documents/workspace/isONcorrect/data/results/sim_depth_abundance.csv ~/tmp/ISONCORRECT/RESULTS_24_12_19/simulated/abundance_plot.pdf


rule zip_plots:
    input: plot1 = rules.plots_full.output.plot, 
           plot2 = rules.plots_subsampling.output.abundance_plot,
           plot3 = rules.plots_subsampling.output.error_rate_plot
    output: plots_zip = config["ROOT_OUT"] + "/evaluation_simulated/plots.tar.gz"
    run:
        plot_pattern = config["ROOT_OUT"] + "/evaluation_simulated/*.pdf"
        shell("tar -cvzf {output.plots_zip} {plot_pattern}")

# rule evaluate:
#     input: original_reads = rules.subsample.output.subsampled_fastq,
#             corrected_reads = rules.combine_isoncorrect.output.corrected_reads_fastq,
#             original_reads_aligned =  rules.align_original_reads_minimap2.output.original_reads_aligned,
#             corrected_reads_aligned =  rules.align_corrected_reads_minimap2.output.corrected_reads_aligned,
#             clusters_tsv = rules.isONclust.output.clusters,
#             gtf_annotation = config["ANNOTATION"] + "/{dataset}.gtf"  # drosophila v97 gtf annotation
#     output: csv_file =  config["ROOT_OUT"] + "/evaluation_biological/{dataset}/{nr_reads}/results.csv"
#     run:
#         if wildcards.dataset == "SIRV":
#             ref = config["SIRV"]
#         elif wildcards.dataset == "NA12878":
#             ref = config["HG38"]
#         elif wildcards.dataset == "drosophila":
#             ref =  config["drosophila97"]

#         eval_dir = config["ROOT_IN"] + "/scripts/"
#         outfolder = config["ROOT_OUT"] + "/evaluation_biological/{0}/{1}/".format(wildcards.dataset, wildcards.nr_reads)  
#         mkdir_p(outfolder) 
#         if wildcards.dataset == "NA12878":
#             shell("python {eval_dir}/evaluate_real_reads.py  {input.original_reads_aligned}  {input.corrected_reads_aligned} {input.original_reads}  \
#                                                         {input.corrected_reads} {ref} {input.clusters_tsv} {input.gtf_annotation} {outfolder} --load_database")
#         elif wildcards.dataset == "drosophila":
#             shell("python {eval_dir}/evaluate_real_reads.py  {input.original_reads_aligned}  {input.corrected_reads_aligned} {input.original_reads}  \
#                                                         {input.corrected_reads} {ref} {input.clusters_tsv} {input.gtf_annotation} {outfolder}")
#         else:
#             shell("python {eval_dir}/evaluate_real_reads.py  {input.original_reads_aligned}  {input.corrected_reads_aligned} {input.original_reads}  \
#                                                         {input.corrected_reads} {ref} {input.clusters_tsv} {input.gtf_annotation} {outfolder} --infer_genes")


# rule summary:
#     input: all_experiments = expand(rules.evaluate.output.csv_file, dataset=config["DATASETS"], nr_reads=config["NR_READS"])
#     output: summary_table = config["ROOT_OUT"] + "/eval_table.csv"
#     run:
#         shell("> {output.summary_table}")
#         outfile = open(output.summary_table ,"w")
#         for f in input.all_experiments: 
#             nr_reads = f.split('/')[-2]
#             dataset = f.split('/')[-3]
#             for line in open(f, 'r'):
#                 outfile.write("{0},{1},{2}\n".format(nr_reads, dataset, ','.join(n for n in line.strip().split(',')) ))
            
#             #shell('echo -n  {dataset},{nr_reads}, >> {output.summary_table} && cat {f} >> {output.summary_table}')




