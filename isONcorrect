#! /usr/bin/env python

from __future__ import print_function
import os,sys
import argparse

import errno
from time import time
import itertools

# from modules import get_sorted_fastq_for_cluster
# from modules import cluster
# from modules import p_minimizers_shared
import math
import parasail
import re
from collections import deque
from modules import correct_seqs

phred_char_to_p = {chr(i) : min( 10**( - (ord(chr(i)) - 33)/10.0 ), 0.5)  for i in range(128)} # PHRED encoded quality character to prob of error. Need this locally if multiprocessing
phred_char_to_val = {chr(i) : max( (ord(chr(i)) - 33), 3)  for i in range(128)} # PHRED encoded quality character to prob of error. Need this locally if multiprocessing

# print(phred_char_to_val)
'''
    Below code taken from https://github.com/lh3/readfq/blob/master/readfq.py
'''
def readfq(fp): # this is a generator function
    last = None # this is a buffer keeping the last unprocessed line
    while True: # mimic closure; is it a bad idea?
        if not last: # the first record or a record following a fastq
            for l in fp: # search for the start of the next record
                if l[0] in '>@': # fasta/q header line
                    last = l[:-1] # save this line
                    break
        if not last: break
        name, seqs, last = last[1:].replace(" ", "_"), [], None
        for l in fp: # read the sequence
            if l[0] in '@+>':
                last = l[:-1]
                break
            seqs.append(l[:-1])
        if not last or last[0] != '+': # this is a fasta record
            yield name, (''.join(seqs), None) # yield a fasta record
            if not last: break
        else: # this is a fastq record
            seq, leng, seqs = ''.join(seqs), 0, []
            for l in fp: # read the quality
                seqs.append(l[:-1])
                leng += len(l) - 1
                if leng >= len(seq): # have read enough quality
                    last = None
                    yield name, (seq, ''.join(seqs)); # yield a fastq record
                    break
            if last: # reach EOF before reading enough quality
                yield name, (seq, None) # yield a fasta record instead
                break


def mkdir_p(path):
    try:
        os.makedirs(path)
        print("creating", path)
    except OSError as exc:  # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise


def cigar_to_seq(cigar, query, ref):
    cigar_tuples = []
    result = re.split(r'[=DXSMI]+', cigar)
    i = 0
    for length in result[:-1]:
        i += len(length)
        type_ = cigar[i]
        i += 1
        cigar_tuples.append((int(length), type_ ))

    r_index = 0
    q_index = 0
    q_aln = []
    r_aln = []
    for length_ , type_ in cigar_tuples:
        if type_ == "=" or type_ == "X":
            q_aln.append(query[q_index : q_index + length_])
            r_aln.append(ref[r_index : r_index + length_])

            r_index += length_
            q_index += length_
        
        elif  type_ == "I":
            # insertion w.r.t. reference
            r_aln.append('-' * length_)
            q_aln.append(query[q_index: q_index + length_])
            #  only query index change
            q_index += length_

        elif type_ == 'D':
            # deletion w.r.t. reference
            r_aln.append(ref[r_index: r_index + length_])
            q_aln.append('-' * length_)
            #  only ref index change
            r_index += length_
        
        else:
            print("error")
            print(cigar)
            sys.exit()

    return  "".join([s for s in q_aln]), "".join([s for s in r_aln])


def parasail_block_alignment(s1, s2, k, match_id, x_acc = "", y_acc = "", match_score = 2, mismatch_penalty = -2, opening_penalty = 5, gap_ext = 1, ends_discrepancy_threshold = 0):
    user_matrix = parasail.matrix_create("ACGT", match_score, mismatch_penalty)
    result = parasail.sg_trace_scan_16(s1, s2, opening_penalty, gap_ext, user_matrix)
    if result.saturated:
        print("SATURATED!")
        result = parasail.sg_trace_scan_32(s1, s2, opening_penalty, gap_ext, user_matrix)
    if sys.version_info[0] < 3:
        cigar_string = str(result.cigar.decode).decode('utf-8')
    else:
        cigar_string = str(result.cigar.decode, 'utf-8')
    
    s1_alignment, s2_alignment = cigar_to_seq(cigar_string, s1, s2)

    # Rolling window of matching blocks
    # k=15
    # match_id = int(k*0.8)  1.0 - math.ceil(window_fraction)
    match_vector = [ 1 if n1 == n2 else 0 for n1, n2 in zip(s1_alignment, s2_alignment) ]
    # print("".join([str(m) for m in match_vector]))
    
    match_window = deque(match_vector[:k]) # initialization
    current_match_count = sum(match_window)
    aligned_region = []
    if current_match_count >= match_id:
        aligned_region.append(1)
    else:
        aligned_region.append(0)


    for new_m_state in match_vector[k:]:
        prev_m_state = match_window.popleft()
        current_match_count = current_match_count - prev_m_state + new_m_state 
        match_window.append(new_m_state)
        
        if current_match_count >= match_id:
            aligned_region.append(1)
        else:        
            aligned_region.append(0)

    block_coverage = "".join([str(m) for m in aligned_region + [aligned_region[-1]]*(k-1) ])
    # print(block_coverage)
    # print("Aligned ratio (tot aligned/len(seq1):", sum(aligned_region)/float(len(s1)))
    alignment_ratio = sum(aligned_region)/float(len(s1))
    return s1, s2, s1_alignment, s2_alignment, alignment_ratio, block_coverage

def block_align(origin_seq, seq, origin_qual, qual, args):

    poisson_mean = sum([ qual.count(char_) * phred_char_to_p[char_] for char_ in set(qual)])
    poisson_mean2 = sum([ origin_qual.count(char_) * phred_char_to_p[char_] for char_ in set(origin_qual)])

    error_rate_sum = poisson_mean/float(len(seq)) + poisson_mean2/float(len(origin_seq))  # k = max(int(mean_plus_two_stdvs_q2 + mean_plus_two_stdvs_q1) + 1 + int(len(seq)*args.variant_rate) , 40)
    if error_rate_sum <= 0.01:
        gap_opening_penalty = 5
    elif  0.01 < error_rate_sum <= 0.04:
        gap_opening_penalty = 4
    elif  0.04 < error_rate_sum <= 0.1:
        gap_opening_penalty = 3
    elif  0.1 < error_rate_sum:
        gap_opening_penalty = 2

    match_id_tailored = math.floor((1.0 - error_rate_sum) * args.k)
    s1, s2, s1_alignment, s2_alignment, alignment_ratio, block_coverage  = parasail_block_alignment(seq, origin_seq, args.k, match_id_tailored, opening_penalty = gap_opening_penalty  )
    # print()
    # print("Expected errors:", poisson_mean, poisson_mean2)
    # print(s1_alignment)
    # print(block_coverage)
    # print(s2_alignment)
    # print(alignment_ratio)
    # print()
    return error_rate_sum, s1_alignment, s2_alignment, alignment_ratio, block_coverage


# def update_reference(ref, read_aln, V):
#     """
#         If reference is a sequence of R bases, store a vector V of 2R+1 bases.
#         Update the reference if 3 sequences are suggesting a diff to current ref
#     """

def get_seq_to_acc(S):
    seq_to_acc = {}
    for i, (acc, seq, qual) in S.items():
        if seq in seq_to_acc:
            seq_to_acc[seq].append(acc)
        else: 
            seq_to_acc[seq] = []
            seq_to_acc[seq] = [acc]

    unique_seq_to_acc = {seq: acc_list[0] for seq, acc_list in  seq_to_acc.items() if len(acc_list) == 1 } 
    print("Non-converged (unique) sequences left:", len(unique_seq_to_acc))
    return seq_to_acc


def main(args):

    print("started sorting seqs")
    start = time()
    reads = { i : (acc, seq, qual) for i, (acc, (seq, qual)) in enumerate(readfq(open(args.fastq, 'r')))}
    # read_array = [ (i, 0, acc, seq, qual, float(acc.split("_")[-1])) for i, (acc, (seq, qual)) in enumerate(readfq(open(sorted_reads_fastq_file, 'r')))]
    print("Correcting {0} reads.".format(len(reads)))
    start = time()

    origin_acc, origin_seq, origin_qual = reads[0]

    qual_dict = {origin_acc : [ phred_char_to_val[q] for q in origin_qual] }    
    block_dict = {origin_acc : [ 1 for q in origin_qual] }    
    seq_to_acc = get_seq_to_acc(reads)
    partition = {}

    ### Augment reference starting with origin as basis
    reference_seq = origin_seq
    reference_qual = origin_qual
    prev_ref = reference_seq
    for i in range(1, len(reads)):
        print(i)
        acc, seq, qual = reads[i]
        error_rate_sum, read_alignment, reference_alignment, alignment_ratio, block_coverage = block_align(reference_seq, seq, reference_qual, qual, args)

        # print()
        # print(read_alignment)
        # print(reference_alignment)
        # print(reference_qual)

        # print([list(_) for ch, g in itertools.groupby(reference_alignment)])
        curr_ref_pos = 0
        curr_aln_pos = 0
        new_ref = []
        new_quals = []
        for ch, g in itertools.groupby(reference_alignment):
            l = list(g)
            if l[0] == "-" and len(l) > 10 : # add quality values of read check here
                # print(l)
                read_pos = len([read_char for read_char in read_alignment[:curr_aln_pos] if read_char != "-"])
                q_part = qual[read_pos:read_pos+ len(l)]
                # print([round(phred_char_to_p[q_v], 4) for q_v in q_part])
                print("AVG:", sum([phred_char_to_p[q_v] for q_v in q_part])/float(len(q_part)))
                print(read_alignment[curr_aln_pos: curr_aln_pos + len(l)] )
                if sum([phred_char_to_p[q_v] for q_v in q_part])/float(len(q_part)) < 0.2:
                    new_ref.append(read_alignment[curr_aln_pos: curr_aln_pos + len(l)] )
                    new_quals.append(qual[read_pos:read_pos+ len(l)])
            if l[0] != "-":
                new_ref.append(l)
                new_quals.append(reference_qual[curr_ref_pos : curr_ref_pos + len(l)])
                curr_ref_pos += len(l)

            curr_aln_pos += len(l)

        # for base in reference_alignment
        # print(len(reference_seq), curr_ref_pos)
        # print(len(reference_alignment), curr_aln_pos)
        reference_seq = "".join([c for sublist in new_ref for c in sublist])
        reference_qual = "".join([c for sublist in new_quals for c in sublist])
        print(len(reference_seq), len(reference_qual)) 
        
        if i > 1 and (prev_ref != reference_seq or len(partition) == 10):
            if len(partition) < 3:
                pass
                print("lol", len(partition))
            else:
                print("Using {0} to update consensus".format(len(partition)))
                new_reference_seq = correct_seqs.update_reference(prev_ref, partition, seq_to_acc, qual_dict)
                error_rate_sum, new_ref_alignment, tmp_ref_alignment, aln_ratio, bl_cov = block_align(prev_ref, new_reference_seq, reference_qual, qual, args)
                print(new_ref_alignment)
                print(reference_alignment)
            
            prev_ref = reference_seq
            partition = {} 
            # reference_seq = new_reference_seq

        if i == 1 and reference_seq != prev_ref:
            prev_ref = reference_seq
            partition = {} 
            print("HERE")
            continue

        
        mismatches = len([ 1 for n1, n2 in zip(reference_alignment,read_alignment) if n1 != n2 and n1 != "-" and n2 != "-" ])
        matches = len([ 1 for n1, n2 in zip(reference_alignment,read_alignment) if n1 == n2 and n1 != "-"])
        indels = len(reference_alignment) - mismatches - matches
        partition[seq] = (matches+indels, reference_alignment, read_alignment, len(seq_to_acc[seq]))

        # print(reference_seq)
        # print(origin_seq)
    print(reference_seq)

    sys.exit()

    # correct all seqs w.r.t. origin consensus

    for i in range(0, len(reads)):
        acc, seq, qual = reads[i]
        error_rate_sum, read_alignment, origin_alignment, alignment_ratio, block_coverage = block_align(reference_seq, seq, reference_qual, qual, args)
        
        # add positons that was aligned to origin here to calculate total coverge over positions
        # start here
        ### get this from block coverage
        ### Add new segments to a new artificially constructed "representative". 
        ### Longest block segment between each two nucleotides in the origin_repr sequence

        mismatches = len([ 1 for n1, n2 in zip(origin_alignment,read_alignment) if n1 != n2 and n1 != "-" and n2 != "-" ])
        matches = len([ 1 for n1, n2 in zip(origin_alignment,read_alignment) if n1 == n2 and n1 != "-"])
        indels = len(origin_alignment) - mismatches - matches

        # print([ phred_char_to_val[q] for q in qual])
        qual_dict[acc] =  [ phred_char_to_val[q] for q in qual]
        # block_dict[acc] =  [ phred_char_to_val[q] for q in block_coverage]
        partition[seq] = (matches+indels, origin_alignment, read_alignment, len(seq_to_acc[seq]))

    S_prime_partition, S_prime_quality_vector = correct_seqs.correct_to_consensus(reference_seq, partition, seq_to_acc, qual_dict)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="De novo clustering of long-read transcriptome reads", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--version', action='version', version='%(prog)s 0.0.2')

    parser.add_argument('--fastq', type=str,  default=False, help='Path to consensus fastq file(s)')
    parser.add_argument('--t', dest="nr_cores", type=int, default=8, help='Number of cores allocated for clustering')

    parser.add_argument('--ont', action="store_true", help='Clustering of ONT transcript reads.')
    parser.add_argument('--isoseq', action="store_true", help='Clustering of PacBio Iso-Seq reads.')

    parser.add_argument('--k', type=int, default=15, help='Kmer size')
    parser.add_argument('--w', type=int, default=50, help='Window size')
    parser.add_argument('--outfolder', type=str,  default=None, help='A fasta file with transcripts that are shared between samples and have perfect illumina support.')
    # parser.add_argument('--pickled_subreads', type=str, help='Path to an already parsed subreads file in pickle format')
    # parser.set_defaults(which='main')
    args = parser.parse_args()

    if args.ont and args.isoseq:
        print("Arguments mutually exclusive, specify either --isoseq or --ont. ")
        sys.exit()
    elif args.isoseq:
        args.k = 15
        args.w = 50
    elif args.ont:
        args.k = 13
        args.w = 20


    if len(sys.argv)==1:
        parser.print_help()
        sys.exit()
    if not args.fastq and not args.flnc and not  args.ccs:
        parser.print_help()
        sys.exit()


    if args.outfolder and not os.path.exists(args.outfolder):
        os.makedirs(args.outfolder)


    # edlib_module = 'edlib'
    parasail_module = 'parasail'
    # if edlib_module not in sys.modules:
    #     print('You have not imported the {0} module. Only performing clustering with mapping, i.e., no alignment.'.format(edlib_module))
    if parasail_module not in sys.modules:
        print('You have not imported the {0} module. Only performing clustering with mapping, i.e., no alignment!'.format(parasail_module))
        sys.exit(1)
    if 100 < args.w or args.w < args.k:
        print('Please specify a window of size larger or equal to k, and smaller than 100.')
        sys.exit(1)

    main(args)

