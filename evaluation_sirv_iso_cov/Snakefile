"""
    snakemake --keep-going -j 999999 --cluster "sbatch --exclude={cluster.exclude} --mem {cluster.mem} -c {cluster.cpus-per-task} -N {cluster.Nodes}  -t {cluster.runtime} -J {cluster.jobname} --mail-type={cluster.mail_type} --mail-user={cluster.mail}" --cluster-config cluster.json --configfile experiments.json --latency-wait 100 --verbose -n

    
    # BIOLOGICAL

    # Subsample reads from original data


    # running isONclust/isONclust2
    1. going from original reads to clusters
    2. from cluster file to fastq files


    ### Running isONcorrect
    1. From cluster fasta to corrected reads
    2. Merge all corrected read clusters

    ### Run evaluation looking for read error rate againse reference (and eventually splice site classification)

    # SIMULATED

    ### simulation evalautions
    4. Basing exon correction plot with error rate

    5. Join everything to table


    # target rules:

"""

shell.prefix("set -o pipefail; ")
# configfile: "experiments.json"

wildcard_constraints:
    nr_reads="[\d]+",

####################################################
########## standard python functions ###############
####################################################

import re
import os
import errno
import shutil
import glob

def mkdir_p(path):
    print("creating", path)
    try:
        os.makedirs(path)
    except OSError as exc:  # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise

rule all:
   input:  config["ROOT_OUT"] + "/sirv_iso_cov.csv"


rule sirv_subsample:
    input:  fastq = config["ROOT_OUT"] + "/data/SIRV/all_fl_reads.fq",
    output: subsampled_fastq =  config["ROOT_OUT"] + "/data/SIRV_iso_cov/{nr_isoforms}/{nr_reads}/{exp_id}.fq"
    run:

        inbase= config["ROOT_IN"]
        mkdir_p(config["ROOT_OUT"] + "/data/SIRV_iso_cov/{0}/{1}/".format(wildcards.nr_reads, wildcards.exp_id ) )
        alignments = config["ROOT_OUT"] + "/evaluation_biological/SIRV/999/original_to_transcripts.sam" # obtained from full experiments pipeline in "evaluation" folder
        # ref = config["SIRV_TRANSCRIPTOME_DISTINCT"]
        shell("python {inbase}/evaluation_sirv/sirv_subsample_depth.py {input.fastq} {alignments} {output.subsampled_fastq} {wildcards.nr_reads}")


rule isONclust:
    input:  fastq = rules.sirv_subsample.output,
    output: time_and_mem =  config["ROOT_OUT"] + "/time_and_mem/{dataset}/{nr_reads}/{exp_id}/time_and_mem.txt",
            clusters = config["ROOT_OUT"] + "/clustering/{dataset}/{nr_reads}/{exp_id}/final_clusters.tsv" 
    run:
        time = config["GNUTIME"]
        mkdir_p(config["ROOT_OUT"] + "/time_and_mem/{0}/{1}/{2}/".format(wildcards.dataset, wildcards.nr_reads, wildcards.exp_id ) )
        outfolder = config["ROOT_OUT"] + "/clustering/{0}/{1}/{2}/".format(wildcards.dataset, wildcards.nr_reads, wildcards.exp_id)
        mkdir_p(outfolder)
        # shell("source activate py36")
        shell("{time} python /galaxy/home/ksahlin/prefix/source/isONclust/isONclust --k 13 --w 20  --q 4.0 --fastq {input.fastq}  --outfolder {outfolder}  2>&1 | tee {output.time_and_mem}")
    

rule clusters_to_fastq:
    input: fastq = rules.isONclust.input.fastq,
            clusters = rules.isONclust.output.clusters
    output: flag = config["ROOT_OUT"] + "/clustering/{dataset}/{nr_reads}/{exp_id}/rule_complete.txt"  
             #"/nfs/brubeck.bx.psu.edu/scratch4/ksahlin/isONclust/mouse_ont_min_phred_q6/fastq_clusters/{clusterid}.fastq"
    run:
        time = config["GNUTIME"]
        # shell("source activate py36")
        outfolder = config["ROOT_OUT"] + "/clustering/{0}/{1}/{2}/fastq/".format(wildcards.dataset, wildcards.nr_reads, wildcards.exp_id)
        shell("{time} python /galaxy/home/ksahlin/prefix/source/isONclust/isONclust write_fastq --clusters {input.clusters} --fastq {input.fastq} --outfolder {outfolder} --N 1")
        shell("touch {output.flag}")


rule isoncorrect:
    input:  rules.clusters_to_fastq.output.flag
    output:  flag = config["ROOT_OUT"] + "/correction/{dataset}/{nr_reads}/{exp_id}/rule_complete.txt" 
    run: 
        # outfolder = "/nfs/brubeck.bx.psu.edu/scratch4/ksahlin/isoncorrect/mouse_ont_min_phred_q6/fastq_clusters/{0}".format(wildcards.clusterid)
        # shell("python /galaxy/home/ksahlin/prefix/source/isONcorrect/isONcorrect --fastq {input.reads}  --outfolder {outfolder} ")
        # shell("source activate py36")
        outfolder = config["ROOT_OUT"] + "/correction/{0}/{1}/{2}/".format(wildcards.dataset, wildcards.nr_reads, wildcards.exp_id)   
        infolder =  config["ROOT_OUT"] + "/clustering/{0}/{1}/{2}/fastq/".format(wildcards.dataset, wildcards.nr_reads, wildcards.exp_id) 
        isoncorrect_dir = config["ROOT_IN"]

        shell("python {isoncorrect_dir}/run_isoncorrect  --fastq_folder {infolder}  --outfolder {outfolder} --set_w_dynamically --t 2  --xmax 80")
        shell("touch {output.flag}")


rule combine_isoncorrect:
    input: rules.isoncorrect.output.flag
    output: corrected_reads_fastq =  config["ROOT_OUT"] + "/correction/{dataset}/{nr_reads}/{exp_id}/isONcorrect.fq"
    run:
        # all_clusters_fastq = expand('/nfs/brubeck.bx.psu.edu/scratch4/ksahlin/isoncorrect/mouse_ont_min_phred_q6/fastq_clusters/{clusterid}/corrected_reads.fastq', clusterid=[str(i) for i in range(0,62747)])
        shell("> {output.corrected_reads_fastq}")
        for f in glob.glob(  config["ROOT_OUT"] + '/correction/{0}/{1}/{2}/*/corrected_reads.fastq'.format(wildcards.dataset, wildcards.nr_reads, wildcards.exp_id)):
            shell('cat {f} >> {output.corrected_reads_fastq}')


rule evaluate_sirv_subsample:
    input: original_reads = rules.sirv_subsample.output.subsampled_fastq,
            corrected_reads = rules.combine_isoncorrect.output.corrected_reads_fastq,
    output: csv_file =  config["ROOT_OUT"] + "/evaluation_biological/{dataset}/{nr_reads}/{exp_id}/results_per_read_to_transcriptome.csv"
    run:
        ref = config["SIRV_TRANSCRIPTOME_DISTINCT"]
        eval_dir = config["ROOT_IN"] + "/scripts/"
        outfolder = config["ROOT_OUT"] + "/evaluation_biological/{0}/{1}/{2}".format(wildcards.dataset, wildcards.nr_reads, wildcards.exp_id)  
        mkdir_p(outfolder) 
        
        orig_reads_aligned = config["ROOT_OUT"] + "/evaluation_biological/{0}/{1}/{2}/original_to_transcripts.sam".format(wildcards.dataset, wildcards.nr_reads,wildcards.exp_id) 
        corr_reads_aligned = config["ROOT_OUT"] + "/evaluation_biological/{0}/{1}/{2}/corrected_to_transcripts.sam".format(wildcards.dataset, wildcards.nr_reads,wildcards.exp_id) 

        shell("/usr/bin/time -v  minimap2 --eqx -t 8 -a -k8 -w1 {ref} {input.original_reads} >  {orig_reads_aligned} ")
        shell("/usr/bin/time -v  minimap2 --eqx -t 8 -a -k8 -w1 {ref} {input.corrected_reads} >  {corr_reads_aligned} ")
        shell("python {eval_dir}/evaluate_sirv_to_transcriptome.py  {orig_reads_aligned}  {corr_reads_aligned} {input.original_reads}  \
                                                        {input.corrected_reads} {ref} {outfolder}")

rule sirv_summary:
    input: all_experiments = expand(rules.evaluate_sirv_subsample.output.csv_file, dataset=["SIRV_iso_cov"], nr_reads = list(range(1,101)), exp_id = list(range(1,11)) )
    output: summary_table = config["ROOT_OUT"] + "/sirv_iso_cov.csv"
    run:
        shell("> {output.summary_table}")
        outfile = open(output.summary_table ,"w")
        for i, f in enumerate(input.all_experiments): 
            
            #write header
            if i == 0:
                outfile.write("experiment_id,nr_reads,acc,read_type,ins,del,subs,matches,error_rate,read_length,aligned_length,chr_id\n")
                # outfile.write("{0},{1},{2}\n".format(nr_reads, dataset, ','.join(n for n in line.strip().split(',')) ))

            exp_id = f.split('/')[-2]
            nr_reads = f.split('/')[-3]
            for j, line in enumerate(open(f, 'r')):
                # ignore header
                if j == 0:
                    continue
                
                outfile.write("{0},{1},{2}\n".format(exp_id, nr_reads, ','.join(n for n in line.strip().split(',')) ))






