#! /usr/bin/env python

from __future__ import print_function
import os,sys
import argparse

import errno
from time import time
import itertools

import math
import re
from collections import deque
from collections import defaultdict

import edlib

from modules import correct_seqs, create_augmented_reference, align, help_functions

def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)



def get_kmer_minimizers(seq, k_size, w_size):
    # kmers = [seq[i:i+k_size] for i in range(len(seq)-k_size) ]
    w = w_size - k_size
    window_kmers = deque([seq[i:i+k_size] for i in range(w +1)])
    curr_min = min(window_kmers)
    minimizers = [ (curr_min, list(window_kmers).index(curr_min)) ]

    for i in range(w+1,len(seq) - k_size):
        new_kmer = seq[i:i+k_size]
        # updateing window
        discarded_kmer = window_kmers.popleft()
        window_kmers.append(new_kmer)

        # we have discarded previous windows minimizer, look for new minimizer brute force
        if curr_min == discarded_kmer: 
            curr_min = min(window_kmers)
            minimizers.append( (curr_min, list(window_kmers).index(curr_min) + i - w ) )

        # Previous minimizer still in window, we only need to compare with the recently added kmer 
        elif new_kmer < curr_min:
            curr_min = new_kmer
            minimizers.append( (curr_min, i) )

    return minimizers

def get_kmer_maximizers(seq, k_size, w_size):
    # kmers = [seq[i:i+k_size] for i in range(len(seq)-k_size) ]
    w = w_size - k_size
    window_kmers = deque([seq[i:i+k_size] for i in range(w +1)])
    curr_min = max(window_kmers)
    minimizers = [ (curr_min, list(window_kmers).index(curr_min)) ]

    for i in range(w+1,len(seq) - k_size):
        new_kmer = seq[i:i+k_size]
        # updateing window
        discarded_kmer = window_kmers.popleft()
        window_kmers.append(new_kmer)

        # we have discarded previous windows minimizer, look for new minimizer brute force
        if curr_min == discarded_kmer: 
            curr_min = max(window_kmers)
            minimizers.append( (curr_min, list(window_kmers).index(curr_min) + i - w ) )

        # Previous minimizer still in window, we only need to compare with the recently added kmer 
        elif new_kmer > curr_min:
            curr_min = new_kmer
            minimizers.append( (curr_min, i) )

    return minimizers


def get_minimizers_and_positions_compressed(reads, w, k, hash_fcn):
    # 1. homopolymenr compress read and obtain minimizers
    M = {}
    for r_id in reads:
        (acc, seq, qual) = reads[r_id]

        seq_hpol_comp = ''.join(ch for ch, _ in itertools.groupby(seq))

        if hash_fcn == "lex":
            minimizers = get_kmer_minimizers(seq_hpol_comp, k, w)
        elif hash_fcn == "rev_lex":
            minimizers = get_kmer_maximizers(seq_hpol_comp, k, w)

        indices = [i for i, (n1,n2) in enumerate(zip(seq[:-1],seq[1:])) if n1 != n2] # indicies we want to take quality values from to get quality string of homopolymer compressed read 
        indices.append(len(seq) - 1)
        positions_in_non_compressed_sring = [(m, indices[p]) for m, p in minimizers ]
        M[r_id] = positions_in_non_compressed_sring

    return M


def get_minimizers_and_positions(reads, w, k, hash_fcn):
    # 1. homopolymenr compress read and obtain minimizers
    M = {}
    for r_id in reads:
        (acc, seq, qual) = reads[r_id]
        if hash_fcn == "lex":
            minimizers = get_kmer_minimizers(seq, k, w)
        elif hash_fcn == "rev_lex":
            minimizers = get_kmer_maximizers(seq, k, w)

        M[r_id] = minimizers

    return M


# def get_minimizers_comb(reads, M, k, x):
#     reads_to_minimizer_combinations_database = {}
#     M2 = defaultdict(lambda: defaultdict(list))
#     for r_id in M:
#         # print(reads[r_id][1])
#         reads_to_M2[r_id] = [] # defaultdict(list)
#         minimizers = M[r_id]
#         for (m1,p1), (m2, p2) in  minimizers_comb_iterator(minimizers, k, x):
#             # reads_to_M2[r_id][(m1, m2)].append((p1,p2))
#             reads_to_M2[r_id].append( ((m1,p1), (m2,p2)) )
#             M2[(m1, m2)][r_id].append((p1,p2))
#     return reads_to_M2, M2

def get_minimizer_combinations_database(reads, M, k, x_low, x_high):
    M2 = defaultdict(lambda: defaultdict(list))
    tmp_cnt = 0
    forbidden = 'A'*k
    for r_id in M:
        minimizers = M[r_id]
        for (m1,p1), m1_curr_spans in  minimizers_comb_iterator(minimizers, k, x_low, x_high):
            for (m2, p2) in m1_curr_spans:
                if m2 == m1 == forbidden:
                    # print("HERE!", p1,p2)
                    continue

                tmp_cnt +=1
                M2[m1][m2].append((r_id, p1, p2))

    print(tmp_cnt, "MINIMIZER COMBINATIONS GENERATED")

    avg_bundance = 0
    singleton_minimzer = 0
    cnt = 1
    abundants=[]
    for m1 in list(M2.keys()):
        for m2 in list(M2[m1].keys()):
            if len(M2[m1][m2]) > 1:
                avg_bundance += len(M2[m1][m2])
                cnt +=1
            else:
                del M2[m1][m2]
                singleton_minimzer += 1

            if len(M2[m1][m2]) > len(reads):
                abundants.append((m1,m2, len(M2[m1][m2])))
                if m2 == forbidden: # poly A tail
                    del M2[m1][m2]
    for m1,m2,ab in sorted(abundants, key=lambda x: x[2], reverse=True):
        print("Too abundant:", m1, m2, ab, len(reads))

    print("Average abundance for non-unique minimizer-combs:", avg_bundance/float(cnt))
    print("Number of singleton minimizer combinations filtered out:", singleton_minimzer)
    return M2



def minimizers_comb_iterator(minimizers, k, x_low, x_high):
    # print("read")
    for i, (m1, p1) in enumerate(minimizers[:-1]):
        m1_curr_spans = []
        for j, (m2, p2) in enumerate(minimizers[i+1:]):
            if x_low < p2 - p1 and p2 - p1 <= x_high:
                m1_curr_spans.append( (m2, p2) )
                # yield (m1,p1), (m2, p2) 
            elif p2 - p1 > x_high:
                break
        yield (m1, p1), m1_curr_spans[::-1]


# def time_test(sorted_left_reads, sorted_right_reads, relevant_reads, r_id, k, x):
#     curr_min_r_pos = 0
#     for l_pos in sorted_left_reads:
#         tmp_add = 0
#         for j, r_pos in enumerate(sorted_right_reads[curr_min_r_pos:]):
#             if k < r_pos - l_pos <= x:
#                 relevant_reads[r_id].append((l_pos, r_pos))
#             elif l_pos > r_pos:
#                 tmp_add = j 
#                 continue
#             elif r_pos > l_pos + x:
#                 break
#         curr_min_r_pos += tmp_add   


# def get_relevant_reads(M2, m1, m2):
#     return M2[m1][m2]



def edlib_alignment(x, y, k):
    # if i == 100 and j % 1000 == 0:
    #     print("Edlib processed alignments: {0}".format(j+1))

    result = edlib.align(x,y, "NW", 'dist', k) # , task="path")
    ed = result["editDistance"]
    # locations = result["locations"]
    return ed #, locations


def get_context_offset(vector, k):
    nuc_obs = 0
    if not vector:
        return 0
    for offset, n in enumerate(vector):
        if n != '-':
            nuc_obs += 1

        if nuc_obs >= k:
            break
    return offset

from modules import kmer_analysis

def get_best_corrections(curr_best_seqs, k_size, outfolder, v_depth_ratio_threshold = 0.1, max_seqs_to_spoa = 200):
    weight = len(curr_best_seqs)
    # r_seq = curr_best_seqs["curr_read"][0]
    # print(r_seq)
    # print(curr_best_seqs)
    # sys.exit()
    #### TMP
    #### TMP
    reads_path = open(os.path.join(outfolder, "reads_tmp.fa"), "w")

    # for q_id, (s_tmp, s_qual, pos1, pos2) in curr_best_seqs.items():
    for i, (q_id, (seq, qual, pos1, pos2)) in enumerate(curr_best_seqs.items()):
        if i > max_seqs_to_spoa:
            break
        # assert len(list_of_spans) == 1
        # for (seq, qual, pos1, pos2) in list_of_spans:
            # print(q_id, str(pos1), str(pos2))
        reads_path.write(">{0}\n{1}\n".format(str(q_id)+str(pos1)+str(pos2), seq))
    reads_path.close()
    spoa_ref = create_augmented_reference.run_spoa(reads_path.name, "/dev/null", os.path.join(outfolder,"spoa_tmp.fa"), "spoa", "/dev/null")
    

    ######### Adjust spoa sequence ###############
    #################################################
    #################################################

    start_anchor = curr_best_seqs["curr_read"][0][:k_size]
    stop_anchor = curr_best_seqs["curr_read"][0][-k_size:]
    start_index = spoa_ref.find(start_anchor) 
    stop_index = len(spoa_ref) - spoa_ref[::-1].find(stop_anchor[::-1]) 
    # print(start_index, stop_index)
    if start_index >=0 and stop_index <= len(spoa_ref):
        spoa_ref_mod = spoa_ref[start_index: stop_index]
        if spoa_ref != spoa_ref_mod:
            print(spoa_ref)
            print(spoa_ref_mod)
        # sys.exit()
        spoa_ref = spoa_ref_mod
    
    # kmer_abundance_per_pos, DBG = kmer_analysis.get_kmer_read_pos_on_ref(curr_best_seqs, spoa_ref, k_size)
    
    # DBG = kmer_analysis.get_kmer_read_pos_on_ref(curr_best_seqs, spoa_ref, k_size)
    
    # if spoa_ref[:k_size] != start_anchor or spoa_ref[-k_size:] != stop_anchor:
    #     print("ENDS NOT EQUAL")
    #     print(start_anchor, stop_anchor)
    #     print(spoa_ref)
    #     print()

    # solve_w_dbg = False
    # for i in range(len(spoa_ref)-k_size+1):
    #     kmer = spoa_ref[i:i+k_size] 
    #     if kmer in DBG:
    #         pass
    #         # print(DBG[kmer], kmer)
    #     else:
    #         # print(kmer, 0)
    #         solve_w_dbg = True
    # if solve_w_dbg:
    #     correction = kmer_analysis.traverse(DBG, start_anchor, stop_anchor, max_depth = 100)
    #     spoa_abundances = [ DBG[spoa_ref[i:i+k_size]]  for i in range(len(spoa_ref)-k_size+1) ]
    #     print(spoa_abundances)
    #     print()
    #     print(spoa_ref, "spoa", len(curr_best_seqs))
    #     print(correction, "kmer_path", len(curr_best_seqs))
    #     print()
    #     for i, (q_id, (seq, qual, pos1, pos2)) in enumerate(curr_best_seqs.items()):
    #         print(seq)
    #     print()
    #     if correction:
    #         # pass
    #         spoa_ref = correction
    #     # sys.exit()

    # ### get low supported regions within spoa ref
    # non_supported_regions = []
    # support = True
    # last_support = -1
    # for i in kmer_abundance_per_pos:
    #     if kmer_abundance_per_pos[i]:
    #         kmer_max, max_ab = max(kmer_abundance_per_pos[i].items(), key= lambda x: x[1])
    #         if spoa_ref[i:i+k_size] == kmer_max: #in kmer_abundance_per_pos[i]:
    #             if support == False:
    #                 if last_support == -1:
    #                     non_supported_regions.append( (0, i, start_anchor, kmer_max) )
    #                 else:                    
    #                     non_supported_regions.append( (last_support, i, last_kmer, kmer_max) )
    #             last_support = i
    #             last_kmer = kmer_max
    #             support = True
    #         else:
    #             support = False
    #     else:
    #         support = False

    # if support == False and last_support < i -1:
    #     non_supported_regions.append( (last_support, i, last_kmer, stop_anchor) ) # the very end

    #     #     if max_ab >= kmer_abundance_per_pos[i][spoa_ref[i:i+k_size]] and spoa_ref[i:i+k_size] != kmer_max:
    #     #         supported_regions.append(i)
    #     #         print("Kmer present in DB but lower coverage, at position:", i, spoa_ref[i:i+k_size], kmer_abundance_per_pos[i][spoa_ref[i:i+k_size]], kmer_max, max_ab  )
    #     # else:
    #     #     supported_regions.append(i)
    #     #     # print("Kmer not even present", i, spoa_ref[i:i+k_size], kmer_max, max_ab  )
    # if non_supported_regions:
    #     corr_spoa_ref = []
    #     print()
    #     # print(non_supported_regions, len(spoa_ref))
    #     for start, stop, kmer1, kmer2 in non_supported_regions:
    #         # print(kmer1, kmer2)
    #         # print(spoa_ref[start:start+k_size], spoa_ref[stop:stop+k_size])
    #         # print(spoa_ref)

    #         if start > 0 and stop < len(spoa_ref)-k_size:
    #             assert kmer1 in spoa_ref[start:start+k_size] and kmer2 in spoa_ref[stop:stop+k_size]
    #         correction = kmer_analysis.traverse(DBG, kmer1, kmer2, max_depth = 40)
    #         if correction:
    #             corr_spoa_ref.append((start, stop, correction))

    #     # corr_spoa_ref = "".join([])
    #     print()
    #     if corr_spoa_ref:
    #         tmp = [seq[0 : corr_spoa_ref[0][0]] ]
    #         for cnt, (start_, stop_, seq_segment) in enumerate(corr_spoa_ref):
    #             tmp.append(seq_segment)
    #             # print(tmp)
    #             # if math.fabs( (stop_ - start_) - len(seq_segment)) > 20:
    #             #     print("Structural correction", stop_ - start_, len(seq_segment))
    #             #     print(seq_segment)
    #             #     print(seq[start_: stop_])

    #             if cnt == len(corr_spoa_ref) - 1:
    #                 tmp.append( seq[ stop_ + k_size : ] )
    #             else:
    #                 # print(cnt)
    #                 tmp.append( seq[ stop_ + k_size: corr_spoa_ref[cnt+1][0]] )
                            
    #         spoa_corr = "".join([s for s in tmp])
    #     # try take a maximum supported path from anchor to anchor

    #         print()
    #         print(start_anchor, stop_anchor)
    #         print(spoa_corr)
    #         print(spoa_ref)
    #         print()
    #         spoa_ref = spoa_corr

    ##############################################################################
    #############################################################################
    #############################################################################

    # for i in kmer_abundance_per_pos:
    #     if not kmer_abundance_per_pos[i]:
    #         print("Empty?")
    #         for j in kmer_abundance_per_pos:
    #             print(j,  kmer_abundance_per_pos[j])
    #             print(spoa_ref, len(spoa_ref))
    #             sys.exit()
    #     else:    
    #         kmer_max, max_ab = max(kmer_abundance_per_pos[i].items(), key= lambda x: x[1])

    #         if kmer_max == spoa_ref[i:i+k_size]:
    #             if corrected_kmer_context_path:
    #                 print(len(spoa_path), len(corrected_kmer_context_path), spoa_path,corrected_kmer_context_path)
    #                 assert len(spoa_path) == len(corrected_kmer_context_path)
    #                 is_valid_path = all([k[1:] == k_fw[:-1] for k,k_fw in zip(corrected_kmer_context_path[:-1], corrected_kmer_context_path[1:]) ])
    #                 if is_valid_path:
    #                     for k_tmp in corrected_kmer_context_path:
    #                         corr_spoa.append(k_tmp)
    #                 else:
    #                     print("NOT VALID PATH")
    #                     for k_tmp in spoa_path:
    #                         corr_spoa.append(k_tmp)
                
    #             corr_spoa.append(kmer_max)
    #             corrected_kmer_context_path = [] # reset local correction
    #             spoa_path = []
    #         else:
    #             spoa_path.append(spoa_ref[i:i+k_size])
    #             if spoa_ref[i:i+k_size] in kmer_abundance_per_pos[i]:
    #                 if max_ab > kmer_abundance_per_pos[i][spoa_ref[i:i+k_size]]:
    #                     print("Kmer present in DB but lower coverage, at position:", i, spoa_ref[i:i+k_size], kmer_abundance_per_pos[i][spoa_ref[i:i+k_size]], kmer_max, max_ab  )
    #                     corrected_kmer_context_path.append(kmer_max)
    #                 else:
    #                   pass  
    #             else:
    #                 print("Kmer not even present", i, spoa_ref[i:i+k_size], kmer_max, max_ab  )
    #                 corrected_kmer_context_path.append(kmer_max)

    # corr_spoa = corr_spoa[0] + "".join([km[-1] for km in corr_spoa[1:] ])
    # print(spoa_ref, len(spoa_ref),i)
    # print(corr_spoa, len(corr_spoa), i)
    # print()
    # print()
    #############################################

    # print(kmer_abundance_per_pos)
    # sys.exit()

    # print()
    # print(spoa_ref)
    #### TMP
    #### TMP
    
    partition = {"ref" : (0, spoa_ref, spoa_ref, 1)}
    # partition = {"ref" : (0, r_seq, r_seq, 1)}
    # for q_id, (s_tmp, s_qual, pos1, pos2) in curr_best_seqs.items():
    for q_id, (seq, qual, pos1, pos2) in curr_best_seqs.items():
        # for (seq, qual, pos1, pos2) in list_of_spans:
        # if s_tmp == r_seq:
        #     continue
        res = edlib.align(seq, spoa_ref, task="path", mode="NW")
        # result_read_to_ref = edlib.align(seq, ref_tmp, task="path", mode="HW")
        # print(result_read_to_ref)
        cigar_string = res["cigar"]
        read_alignment, ref_alignment = help_functions.cigar_to_seq(cigar_string, seq, spoa_ref)
        partition[(q_id, pos1, pos2)] = (res["editDistance"], ref_alignment, read_alignment, 1)
        # print(read_alignment, ref_alignment)
    alignment_matrix = correct_seqs.create_multialignment_matrix(partition)
    # print(alignment_matrix)
    # for r_tmp, aln_list in alignment_matrix.items():
    #     print(aln_list)
    nr_columns = len(alignment_matrix["ref"])
    PFM = [{"A": 0, "C": 0, "G": 0, "T": 0, "U" : 0, "-": 0, "N": 0} for j in range(nr_columns)]
    for r_tmp, aln_list in alignment_matrix.items():
        if r_tmp == "ref":
            continue
        for j, n in enumerate(aln_list):
            PFM[j][n] += 1
    # print(PFM)

    other_corrections = defaultdict(list)
    # read_aln = alignment_matrix["curr_read"]

    # get homopolymer adjustment
    h_tmp = [(len(list(_)), ch ) for ch, _ in itertools.groupby(spoa_ref) ]
    # print(h_tmp)
    # print(spoa_ref)
    h_tmp2 = []
    h_tmp2 = [(c, ch, i) for (c, ch) in h_tmp for i in range(c)]
    # print(h_tmp2)
    assert len(h_tmp2) == len(spoa_ref)
    curr_pos = 0
    h_pol_counts = []
    n_cols = len(h_tmp2)
    for nucl in alignment_matrix["ref"]:
        # print(curr_pos, len(h_tmp2))
        h_pol_counts.append(h_tmp2[min(n_cols - 1,curr_pos)])
        if nucl != "-":
            curr_pos += 1
    # print(alignment_matrix["ref"])
    # print(h_pol_counts)    
    assert len(alignment_matrix["ref"]) == len(h_pol_counts)


    # TODO: potentially add context around substitutions here
    variant_threshold = max(5, len(curr_best_seqs) * v_depth_ratio_threshold)
    # print(variant_threshold, len(curr_best_seqs), spoa_ref)

    ref_aln = alignment_matrix["ref"]
    contexts_per_pos = {}
    for i_tmp in range(len(ref_aln)):
        nuc_obs = 0
        j_tmp = i_tmp
        p1 = get_context_offset(ref_aln[:i_tmp][::-1], k_size)
        p2 = get_context_offset(ref_aln[i_tmp + 1:], k_size)
        # print(p1,p2)
        # print(ref_aln)
        contexts_per_pos[i_tmp] = (i_tmp - p1, i_tmp + p2)

        # print(i_tmp, p1, p2, ref_aln[i_tmp], ref_aln[i_tmp - p1: i_tmp + p2])
    # sys.exit()

    for i, d in enumerate(PFM):
        # calculate reference context here the first thing we do from ref_aln
        ref_nucl = alignment_matrix["ref"][i]
        if ref_nucl != "-":
            for q_id_tuple in alignment_matrix:
                if q_id_tuple == "ref":
                    continue
                read_aln = alignment_matrix[q_id_tuple]
                read_nucl = read_aln[i]
                if read_nucl != "-" and read_nucl != ref_nucl:
                    if variant_threshold <= d[read_nucl]:
                        if ref_aln[contexts_per_pos[i][0]: i] == read_aln[contexts_per_pos[i][0]: i] and ref_aln[i+1: contexts_per_pos[i][1]] == read_aln[i+1: contexts_per_pos[i][1]]: # potential variant position
                            other_corrections[q_id_tuple].append(read_nucl)
                            # print("KEEPING HERE")
                            # print(ref_aln[contexts_per_pos[i][0]: contexts_per_pos[i][1]])
                            # print(read_aln[contexts_per_pos[i][0]:contexts_per_pos[i][1]])
                            # print(variant_threshold, len(curr_best_seqs), "actual ab:", d[read_nucl] )
                            # print()
                            # print()
                        else:
                            # print("FAILED CONTEXT")
                            # print(ref_aln[contexts_per_pos[i][0]: contexts_per_pos[i][1]])
                            # print(read_aln[contexts_per_pos[i][0]: contexts_per_pos[i][1]])
                            # print(variant_threshold, len(curr_best_seqs), read_nucl, ref_nucl, "actual ab:", d[read_nucl] )
                            # print()
                            # print()
                            other_corrections[q_id_tuple].append(ref_nucl)

                    else:
                        other_corrections[q_id_tuple].append(ref_nucl)
                # elif read_nucl != ref_nucl
                else:
                    other_corrections[q_id_tuple].append(ref_nucl)

    # for i, d in enumerate(PFM):
    #     ref_nucl = alignment_matrix["ref"][i]

    #     n_max, count_max = max(d.items(), key = lambda x: x[1])
    #     if n_max == "-" and n_max != ref_nucl:
    #         # c, n_max2 = max([(c,n) for n,c in d.items() if n != "-"])
    #         if h_pol_counts[i][0]*d[ref_nucl] > count_max:
    #             n_max = ref_nucl #h_pol_counts[i][1 ]#n_max2 
    #             # count_max = c
    #             print("Changed - to", n_max, "hpol length:", h_pol_counts[i][0], "ref count:", d[ref_nucl], "prev max:", count_max )

    #     # n_max = alignment_matrix["ref"][i]
    #     # count_max = d[ref_nucl]
    #     variant_threshold = max(2, count_max * v_depth_ratio_threshold) # at least two reads have to have variant to avoid degenerate case
    #     for q_id_tuple in alignment_matrix:
    #         if q_id_tuple == "ref":
    #             continue

    #         read_nucl = alignment_matrix[q_id_tuple][i]
    #         if read_nucl != "-" and read_nucl != n_max:
    #             if variant_threshold <= d[read_nucl]: # potential variant position
    #                 other_corrections[q_id_tuple].append(read_nucl)
    #             else:
    #                 other_corrections[q_id_tuple].append(n_max)
    #         # elif read_nucl != ref_nucl
    #         else:
    #             other_corrections[q_id_tuple].append(n_max)



    other_corrections_final = defaultdict(list)
    for q_id_tuple in other_corrections:
        other_read_corr = other_corrections[q_id_tuple]
        other_read_corr = "".join([n for n in other_read_corr if n != "-"]) #spoa_ref #
        # print(q_id_tuple)
        q_id, q_p1, q_p2 = q_id_tuple
        # print()
        other_corrections_final[q_id].append( (q_p1 + k_size, q_p2, weight, other_read_corr[k_size:-k_size] ))

        if q_id == "curr_read":
            curr_read_corr = "".join([n for n in other_read_corr if n != "-"]) #spoa_ref #

    return curr_read_corr[k_size:-k_size], other_corrections_final



def solve_WIS(all_intervals_sorted_by_finish):
    p = [None]
    v = [None] + [w*(stop-start) for (start, stop, w, _) in all_intervals_sorted_by_finish]
    for j, (start, stop, w, _) in enumerate(all_intervals_sorted_by_finish):
        if j == 0:
            p.append(0)
            continue

        k = j - 1
        while k >= 0:
            if all_intervals_sorted_by_finish[k][1] <= start:
                break
            k -= 1
        p.append(max(0,k))

    # for i, interval in enumerate(all_intervals_sorted_by_finish):
    #     print(i, p[i+1], interval[0], interval[1], interval[2])

    OPT = [0]
    for j in range(1, len(all_intervals_sorted_by_finish) +1):
        OPT.append( max(v[j] + OPT[ p[j] ], OPT[j-1] ) )
    # print(OPT, OPT[-1], all_intervals_sorted_by_finish)

    assert len(p) == len(all_intervals_sorted_by_finish) + 1 == len(v) == len(OPT)

    # Find solution
    # def find_solution(j, opt_indicies): 
    #     if j == 0:
    #         return 
    #     elif v[j] + OPT[p[j]] > OPT[j-1]:
    #         print(j)
    #         opt_indicies.append(j - 1) # we have shifted all indices forward by one so we neew to reduce to j -1 because of indexing in python works
    #         find_solution( p[j], opt_indicies ) 
    #     else:
    #         find_solution(j-1, opt_indicies) 
    # opt_indicies = []
    # find_solution(len(all_intervals_sorted_by_finish), opt_indicies)
    # print("test",opt_indicies)

    # Find solution
    opt_indicies = []
    j = len(all_intervals_sorted_by_finish)
    while j >= 0:
        if j == 0:
            break
        if v[j] + OPT[p[j]] > OPT[j-1]:
            # print(j)
            opt_indicies.append(j - 1) # we have shifted all indices forward by one so we neew to reduce to j -1 because of indexing in python works
            j = p[j]
            # find_solution( p[j], opt_indicies ) 
        else:
            j -= 1
    # print( "hi", opt_indicies, "optimal value:", OPT[-1])
    # assert opt_indicies2 == opt_indicies
    return opt_indicies




# def eval_sim(corr, seq, qual, tot_before, tot_after):
#     true = "AGCAATCAGACTCAAGCATTAAAATACCCGCCCAGAATATGCCATCAAACTGCCTGGCCCGACAAATTAATGCTGAAATTGTTAACTGACCGGAGGTCGACGTACCGACACTAGGAGGAGCGGGACTGTAGGAAAACCACTATCCTTTTC"
#     r1 = edlib.align(true, corr, task="path", mode="NW")
#     ref_alignment1, read_alignment = help_functions.cigar_to_seq(r1["cigar"], true, corr)

#     ins = len([1 for n in ref_alignment1[10:-10] if n == "-"])
#     del_ = len([1 for n in read_alignment[10:-10] if n == "-"])
#     subs =  len([p for p, (n1,n2) in enumerate(zip(read_alignment[10:-10], ref_alignment1[10:-10])) if n1 != "-" and n2 != "-" and n1 != n2 ])
#     tot_after["subs"] += subs
#     tot_after["ins"] += ins
#     tot_after["del"] += del_

#     r2 = edlib.align(true, seq, task="path", mode="NW")
#     ref_alignment2, read_orig_alignment = help_functions.cigar_to_seq(r2["cigar"], true, seq)

#     ins = len([1 for n in ref_alignment2 if n == "-"])
#     del_ = len([1 for n in read_orig_alignment if n == "-"])
#     subs =  r2["editDistance"] - ins - del_
#     tot_before["subs"] += subs
#     tot_before["ins"] += ins
#     tot_before["del"] += del_

# def eval_sim2(corr, seq, qual, tot_before, tot_after):
#     true = "ACCCGGCGGTACCATACTTCCGAACGACATATTAGTACGTGTACGTTTGAGTCGTCATTCGGCATGCGTAATAACGCTTACGTTGATTCTCAACGTGATACACTAGTTGTCGACGTGAGTACGAAGGTCGGTAGTGGAATATGGTCGCACAGGGTCGGACGCAAAGAATTTTTCAAGACAGCCTGAGTGCGGTGCAACAATCGGTGAGTAACCGTGGGCGAAACTTGAAGTCAGGGTATCGATGCATCACGGACATTAATCGAGGCTGTGAAGGCCTCACCCATTGCATAGTCATGCCCG"
#     r1 = edlib.align(true, corr, task="path", mode="NW")
#     ref_alignment1, read_alignment = help_functions.cigar_to_seq(r1["cigar"], true, corr)
#     print(ref_alignment1)
#     print(read_alignment)

#     ins = len([1 for n in ref_alignment1 if n == "-"])
#     del_ = len([1 for n in read_alignment if n == "-"])
#     subs =  len([p for p, (n1,n2) in enumerate(zip(read_alignment, ref_alignment1)) if n1 != "-" and n2 != "-" and n1 != n2 ])
#     tot_after["subs"] += subs
#     tot_after["ins"] += ins
#     tot_after["del"] += del_

#     r2 = edlib.align(true, seq, task="path", mode="NW")
#     ref_alignment2, read_orig_alignment = help_functions.cigar_to_seq(r2["cigar"], true, seq)

#     ins = len([1 for n in ref_alignment2 if n == "-"])
#     del_ = len([1 for n in read_orig_alignment if n == "-"])
#     subs =  r2["editDistance"] - ins - del_
#     tot_before["subs"] += subs
#     tot_before["ins"] += ins
#     tot_before["del"] += del_


def find_most_supported_span(r_id, m1, p1, m1_curr_spans, minimizer_combinations_database, reads, all_intervals, k_size, tmp_cnt, read_complexity_cnt, quality_values_database, already_computed):

    # curr_m_pos = read_min_comb[0][0][1]
    # curr_m_pos2 = read_min_comb[0][1][1]
    acc, seq, qual = reads[r_id]
    curr_best_seqs = {}
    curr_best_seqs_to_id = {}
    # cnt = 0
    for (m2,p2) in m1_curr_spans:
        # print(p1,p2)
        relevant_reads = minimizer_combinations_database[m1][m2]
        seqs = {} #defaultdict(list)
        if len(relevant_reads) >= max(3,len(curr_best_seqs)): #max(3, previously_calculated_regions_read[p2]): # 
            # cnt += 1
            ref_seq = seq[p1  : p2 + k_size]
            ref_qual = qual[p1 : p2 + k_size]            
            p_error_ref = (quality_values_database[r_id][p2 + k_size] - quality_values_database[r_id][p1])/(p2 + k_size - p1)

            seqs["curr_read"] = (ref_seq, ref_qual, p1, p2)
            reads_visited = {}
            for relevant_read_id, pos1, pos2 in relevant_reads:
                if r_id  == relevant_read_id:
                    continue
                
                read_seq = reads[relevant_read_id][1][pos1: pos2 + k_size]
                read_qual = reads[relevant_read_id][2][pos1: pos2 + k_size]
                if read_seq == ref_seq:
                    # seqs[relevant_read_id] = []
                    seqs[relevant_read_id] = (read_seq, read_qual, pos1, pos2)
                    reads_visited[relevant_read_id] = 0
                    already_computed[relevant_read_id] = (p1,p2,pos1,pos2, 0)
                    continue

                
                if relevant_read_id in reads_visited:
                    # print("Prev:", reads_visited[relevant_read_id])
                    # print("Act:", edlib_alignment(ref_seq, read_seq, p_error_sum_thresh*len(ref_seq)) )
                    pass
        # Implement if we see this to recompute all the aligments exact ed here instead!! Thats the only way to guarantee exactly the same
        # or maybe use this traceback to get exact: https://github.com/Martinsos/edlib/pull/132#issuecomment-522258271
                elif relevant_read_id in already_computed:
                    curr_ref_start, curr_ref_end, curr_read_start, curr_read_end, curr_ed = already_computed[relevant_read_id]
                    if (curr_read_start <= pos1 and pos2 <= curr_read_end) and (curr_ref_start <= p1 and p2 <=  curr_ref_end):
                        p_error_read = (quality_values_database[relevant_read_id][pos2 + k_size] - quality_values_database[relevant_read_id][pos1])/(pos2 + k_size - pos1)
                        p_error_sum_thresh = p_error_ref + p_error_read # curr_p_error_sum_thresh*len(ref_seq)
                        read_beg_diff = pos1 - curr_read_start
                        read_end_diff = pos2 - curr_read_end
                        ref_beg_diff = p1 - curr_ref_start
                        ref_end_diff = p2 - curr_ref_end


                        if 0 <= curr_ed + math.fabs(ref_end_diff - read_end_diff) + math.fabs(read_beg_diff - ref_beg_diff)  <= p_error_sum_thresh*len(ref_seq): # < curr_p_error_sum_thresh*len(ref_seq):
                            # print("saved:",pos1, pos2, p1,p2, curr_ref_start, curr_ref_end, curr_read_start, curr_read_end, curr_ed, curr_p_error_sum_thresh)
                            # print("saved", (curr_ref_end - curr_ref_start) - (p2-p1), (curr_read_end - curr_read_start) - (pos2 - pos1))

                            # read_seq = reads[relevant_read_id][1][pos1: pos2 + k_size]
                            # read_qual = reads[relevant_read_id][2][pos1: pos2 + k_size]
                            seqs[relevant_read_id] = (read_seq, read_qual, pos1, pos2)
                            reads_visited[relevant_read_id] = curr_ed + math.fabs(ref_end_diff - read_end_diff) + math.fabs(read_beg_diff - ref_beg_diff)

                            # est = curr_ed + math.fabs(ref_end_diff - read_end_diff) + math.fabs(read_beg_diff - ref_beg_diff)
                            # act = edlib_alignment(ref_seq, read_seq, p_error_sum_thresh*len(ref_seq))
                            # if est != act:
                            #     print("estimated", est)
                            #     print("Actual", act)

                            continue

                    else:
                        pass
                


                p_error_read = (quality_values_database[relevant_read_id][pos2 + k_size] - quality_values_database[relevant_read_id][pos1])/(pos2 + k_size - pos1)
                p_error_sum_thresh = p_error_ref + p_error_read #sum([D[char_] for char_ in read_qual])/len(read_qual) #+ 0.1
                # p_error_sum_thresh = max(4, p_error_ref + p_error_read) #sum([D[char_] for char_ in read_qual])/len(read_qual) #+ 0.1
                # error_cutoff = max(5, p_error_sum_thresh*(max(len(ref_seq), len(read_seq)) -2*k_size) )
                # print(p_error_sum_thresh, p_error_sum_thresh*len(ref_seq),len(ref_seq) )
                editdist = edlib_alignment(ref_seq, read_seq, p_error_sum_thresh*len(ref_seq))

                if editdist == 0:
                    print("Here!")
                tmp_cnt += 1
                if editdist >= 0:    # passing second edit distance check
                    if relevant_read_id in reads_visited: # we have already seen the minimizer combination
                        prev_read_seq, read_read_qual, prev_pos1, prev_pos2 = seqs[relevant_read_id]
                        editdist_prev = edlib_alignment(ref_seq, prev_read_seq, len(ref_seq))
                        tmp_cnt += 1
                        read_complexity_cnt += 1
                        # print("Read already visited lool", "prev:",reads_visited[relevant_read_id], "this:", editdist)
                        # print(seqs[relevant_read_id])
                        # print(p_error_sum_thresh)
                        # print(ref_seq,p1,p2)
                        # print(read_seq, pos1, pos2)
                        # print("current:", editdist, "prev", reads_visited[relevant_read_id])
                        if editdist < editdist_prev:
                            # seqs[relevant_read_id] = []
                            seqs[relevant_read_id] = (read_seq, read_qual, pos1, pos2)
                            reads_visited[relevant_read_id] = editdist
                            already_computed[relevant_read_id] = (p1,p2,pos1,pos2, editdist)
                            # print("REPLACED OLD MATCH")
                        else:
                            # seqs[relevant_read_id] = []
                            seqs[relevant_read_id] = (prev_read_seq, read_read_qual, prev_pos1, prev_pos2)
                            reads_visited[relevant_read_id] = editdist_prev
                            already_computed[relevant_read_id] = (p1,p2,prev_pos1, prev_pos2, editdist_prev)
                    else:
                        seqs[relevant_read_id] = (read_seq, read_qual, pos1, pos2)
                        reads_visited[relevant_read_id] = editdist
                        already_computed[relevant_read_id] = (p1,p2,pos1,pos2, editdist)


            all_intervals.append( (p1 + k_size, p2,  len(seqs), seqs) )
    #     if len(seqs) > len(curr_best_seqs):
    #         curr_best_seqs = seqs
    #         # curr_best_seqs_to_id = seqs_to_id
    #         curr_best_stop = p2


    # if len(curr_best_seqs) > 2:
    #     all_intervals.append( (p1 + k_size, curr_best_stop,  len(curr_best_seqs), curr_best_seqs) )
    
        # for q_id, (read_seq, read_qual, pos1, pos2) in curr_best_seqs.items():
        #     if q_id != "curr_read":
        #         stored_calculated_regions[q_id][pos1] = len(curr_best_seqs)
    return tmp_cnt, read_complexity_cnt

def correct_read(seq, opt_indicies, all_intervals_sorted_by_finish, k_size, outfolder, v_depth_ratio_threshold, max_seqs_to_spoa):
    corr_seq = []
    # print(opt_indicies)
    other_reads_corrected_regions = defaultdict(list)
    first_start = all_intervals_sorted_by_finish[opt_indicies[0]][0]
    final_stop = all_intervals_sorted_by_finish[opt_indicies[-1]][1]
    prev_stop = 0
    for j in opt_indicies:
        # print(all_intervals_sorted_by_finish[j])
        start, stop, weights, instance = all_intervals_sorted_by_finish[j]
        # print(start-7, stop+7, weights)
        if start - k_size > prev_stop and prev_stop > 0:
            # print()
            eprint("Gap in correction:", start-k_size - prev_stop, "between positions:", prev_stop, start, )
            # print()
            # sys.exit()
        prev_stop = stop + k_size

        if isinstance(instance, str): # already corrected
            best_corr = instance
        else:
            best_corr, other_corrections = get_best_corrections(instance, k_size, outfolder, v_depth_ratio_threshold, max_seqs_to_spoa) # store all corrected regions within all reads in large container and keep track when correcting new read to not re-compute these regions     
            for other_r_id, other_corr_regions in other_corrections.items():
                for region in other_corr_regions:
                    other_reads_corrected_regions[other_r_id].append(region)
        # print(seq[start: stop],  best_corr)
        corr_seq.append((start,stop, best_corr))
    # corr_seq = corr_seq
    # print(corr_seq, len(seq))
    eprint(first_start, final_stop)
    tmp = [seq[0 : corr_seq[0][0]] ]
    for cnt, (start_, stop_, seq_segment) in enumerate(corr_seq):
        tmp.append(seq_segment)
        # print(tmp)
        # if math.fabs( (stop_ - start_) - len(seq_segment)) > 20:
        #     print("Structural correction", stop_ - start_, len(seq_segment))
        #     print(seq_segment)
        #     print(seq[start_: stop_])

        if cnt == len(corr_seq) - 1:
            tmp.append( seq[ stop_ : ] )
        else:
            # print(cnt)
            tmp.append( seq[ stop_ : corr_seq[cnt+1][0]] )
                    
    corr = "".join([s for s in tmp])
    return corr, other_reads_corrected_regions


D = {chr(i) : min( 10**( - (ord(chr(i)) - 33)/10.0 ), 0.79433)  for i in range(128)}

def get_qvs(reads):
    quality_values_database = {}
    for r_id in reads:
        (acc, seq, qual) = reads[r_id]
        quality_values_database[r_id] = [0]
        tmp_tot_sum = 0
        for char_ in qual:
            qv = D[char_]
            quality_values_database[r_id].append( tmp_tot_sum + qv )  #= [D[char_] for char_ in qual]
            tmp_tot_sum += qv
    return quality_values_database


def check_max_common_interval(list_of_intervals):
    min_read_pos, max_read_pos = 10000, -1
    corresp_ref_pos1, corresp_ref_pos2 = -1,-1
    for read_pos1, read_pos2, ref_pos1, ref_pos2 in list_of_intervals:
        if read_pos1 < min_read_pos:
            min_read_pos = read_pos1
            corresp_ref_pos1 = ref_pos1
        if read_pos2 > max_read_pos:
            max_read_pos = read_pos2
            corresp_ref_pos2 = ref_pos2

    return min_read_pos, max_read_pos, corresp_ref_pos1, corresp_ref_pos2 

def test_potential_speedup(not_prev_corrected_spans, minimizer_combinations_database, reads, ref_id, m1, p1):
    l_tmp = [ (relevant_read_id, pos1, pos2, p1, p2 ) for (m2,p2) in not_prev_corrected_spans for (relevant_read_id, pos1, pos2) in minimizer_combinations_database[m1][m2] ] 
    d_tmp = defaultdict(list)
    for (relevant_read_id, pos1, pos2, ref_pos1, ref_pos2) in l_tmp:
        d_tmp[relevant_read_id].append( (pos1, pos2, ref_pos1, ref_pos2))
    d_tmp2 = {}
    # calc_max_common_interval(d_tmp[relevant_read_id])
    for relevant_read_id in d_tmp:
        min_read_pos, max_read_pos, corresp_ref_pos1, corresp_ref_pos2  = check_max_common_interval(d_tmp[relevant_read_id])
        # min_ref_coord, max_ref_coord = d_tmp[relevant_read_id][0][-2], d_tmp[relevant_read_id][-1][-1]
        # min_read_coord, max_read_coord = d_tmp[relevant_read_id][0][-4], d_tmp[relevant_read_id][-1][-3]
        if max_read_pos - min_read_pos <= 80 and corresp_ref_pos2 - corresp_ref_pos1 <= 80:
            # print(max_read_pos - min_read_pos, corresp_ref_pos2 - corresp_ref_pos1 )
            d_tmp2[relevant_read_id] = (corresp_ref_pos1, corresp_ref_pos2, min_read_pos, max_read_pos, len(d_tmp[relevant_read_id]))

        else:
            pass
            # print("Not good instance!")
            # print(d_tmp[relevant_read_id])

        # print(d_tmp[relevant_read_id])

    # print(len(d_tmp2))
    for i, relevant_read_id in enumerate(d_tmp2):
        # print("Starting", i)
        if relevant_read_id == ref_id:
            continue
        ref_start, ref_stop, read_start, read_stop, nr_segments = d_tmp2[relevant_read_id]
        ref_seq = reads[ref_id][1][ref_start: ref_stop]
        read_seq =  reads[relevant_read_id][1][read_start: read_stop]
        # print(ref_start, ref_stop, read_start, read_stop, nr_segments)
        # print(read_seq, ref_seq)
        # res = edlib.align(read_seq, ref_seq, task="path", mode="NW")
        # result_read_to_ref = edlib.align(seq, ref_tmp, task="path", mode="HW")
        # print(result_read_to_ref)
        # cigar_string = res["cigar"]
        # print(res)
        # read_alignment, ref_alignment = help_functions.cigar_to_seq(cigar_string, read_seq, ref_seq)
        # print(read_alignment)
        # print(ref_alignment)
    # print("completed")

def main(args):
    start = time()
    reads = { i : (acc, seq, qual) for i, (acc, (seq, qual)) in enumerate(help_functions.readfq(open(args.fastq, 'r')))}
    eprint("Correcting {0} reads.".format(len(reads)))
    max_seqs_to_spoa = args.max_seqs_to_spoa
    if len(reads) <= args.exact_instance_limit:
        args.exact = True
    if args.set_w_dynamically:
        args.w = args.k + min(7, int( len(reads)/500))

    eprint("ARGUMENT SETTINGS:")
    for key, value in args.__dict__.items():
        eprint("{0}: {1}".format(key, value))
        # setattr(self, key, value)
    eprint()


    start = time()
    corrected_reads = {}
    v_depth_ratio_threshold = args.T
    for k_size in range(args.k, args.k +1):
        # DBG, position_DBG = create_augmented_reference.kmer_counter(reads, k_size)
        # print("done createng DB")
        w = args.w
        x_high = args.xmax
        x_low = args.xmin
        for hash_fcn in ["lex"]: # ["lex"]: #  add "rev_lex" # add four others
            if args.compression:
                minimizer_database  = get_minimizers_and_positions_compressed(reads, w, k_size, hash_fcn)
            else:
                minimizer_database  = get_minimizers_and_positions(reads, w, k_size, hash_fcn)

            # reads_to_M2, M2 = get_minimizers_comb(reads, minimizer_database, k_size, args.X)
            minimizer_combinations_database = get_minimizer_combinations_database(reads, minimizer_database, k_size, x_low, x_high)
            quality_values_database = get_qvs(reads)
            # print(minimizer_database)
            eprint("done creating minimizer combinations")

            # print( [ (xx, len(reads_to_M2[xx])) for xx in reads_to_M2 ])
            # sys.exit()
            corrected_reads = {}
            tot_errors_before = {"subs" : 0, "del": 0, "ins": 0}
            tot_errors_after = {"subs" : 0, "del": 0, "ins": 0}
            tot_corr = 0
            previously_corrected_regions = defaultdict(list)
            # stored_calculated_regions = defaultdict(lambda: defaultdict(int))
            tmp_cnt = 0

            for r_id in sorted(reads): #, reverse=True):
                read_min_comb = [ ((m1,p1), m1_curr_spans) for (m1,p1), m1_curr_spans in  minimizers_comb_iterator(minimizer_database[r_id], k_size, x_low, x_high)]
                # print(read_min_comb)
                # sys.exit()
                if args.exact:
                    previously_corrected_regions = defaultdict(list)
                # stored_calculated_regions = defaultdict(list)
        
                #  = stored_calculated_regions[r_id]
                corr_pos = []
                (acc, seq, qual) = reads[r_id]
                # print("starting correcting:", seq)
                all_intervals = []

                # print(r_id, sorted(previously_corrected_regions[r_id], key=lambda x:x[1]))
                read_previously_considered_positions = set([tmp_pos for tmp_p1, tmp_p2, w_tmp, _ in previously_corrected_regions[r_id] for tmp_pos in range(tmp_p1, tmp_p2)])
                if read_previously_considered_positions:
                    eprint("not corrected:", [ (p1_, p2_) for p1_, p2_ in zip(sorted(read_previously_considered_positions)[:-1], sorted(read_previously_considered_positions)[1:]) if p2_ > p1_ + 1 ] )
                else:
                    eprint("not corrected: entire read", )

                if previously_corrected_regions[r_id]:
                    read_previously_considered_positions = set([tmp_pos for tmp_p1, tmp_p2, w_tmp, _ in previously_corrected_regions[r_id] for tmp_pos in range(tmp_p1, tmp_p2)])
                    group_id = 0
                    pos_group = {}
                    sorted_corr_pos = sorted(read_previously_considered_positions)
                    for p1, p2 in zip(sorted_corr_pos[:-1], sorted_corr_pos[1:]):
                        if p2 > p1 + 1:
                           pos_group[p1] = group_id 
                           group_id += 1
                           pos_group[p2] = group_id 
                        else:
                           pos_group[p1] = group_id 
                    if p2 == p1 + 1:
                        pos_group[p2] = group_id 
                else:
                    read_previously_considered_positions= set()
                    pos_group = {}
                # print(pos_group)

                # read_all_pos = set(range(len(seq))) 
                # read_not_corr = read_all_pos - read_previously_considered_positions
                # print(previously_corrected_regions[r_id])
                # print(read_previously_considered_positions)
                
                # print()
                # print(len(read_not_corr), sorted(read_not_corr))
                # print()
                
                # read_prev_corrected_positions2 = defaultdict(set)
                # if len(previously_corrected_regions[r_id]) > 1:
                #     prev_regions = sorted(previously_corrected_regions[r_id], key = lambda x: x[1])
                #     for (tmp_p1, tmp_p2, w_tmp, _ ) in (tmp_p1_2, tmp_p2_2, w_tmp_2, _2 ) zip(prev_regions[r_id][:-1], prev_regions[r_id][1:]):
                #         if tmp_p2 + k_size >= tmp_p1_2:
                #             read_prev_corrected_positions2[tmp_p1] = set(range(tmp_p1, tmp_p2))
                #         else:
                #             read_prev_corrected_positions2[tmp_p1] = set(range(tmp_p1, tmp_p2))
                # print(read_prev_corrected_positions2)

                # print(read_previously_considered_positions)
                already_computed = {}
                read_complexity_cnt = 0
                for (m1,p1), m1_curr_spans in read_min_comb: 
                    # Implement that if any position is not in range of current corrections: then correct!!! Not just start and stop
                    not_prev_corrected_spans = [(m2,p2) for (m2,p2) in m1_curr_spans if not (p1 + k_size in read_previously_considered_positions and p2 - 1 in read_previously_considered_positions) ] 
                    set_not_prev = set(not_prev_corrected_spans)
                    not_prev_corrected_spans2 = [(m2,p2) for (m2,p2) in m1_curr_spans if (m2,p2) not in set_not_prev and (p1 + k_size in pos_group and p2 - 1 in pos_group and pos_group[p1 + k_size] != pos_group[p2 - 1]) ] 
                    # if not_prev_corrected_spans2:
                    #     print("extra spans:", not_prev_corrected_spans2)
                    not_prev_corrected_spans += not_prev_corrected_spans2
                    # print(not_prev_corrected_spans)
                    # not_prev_corrected_spans3 = [(m2,p2) for (m2,p2) in m1_curr_spans if not (p1 + k_size in read_previously_considered_positions and p2 - 1 in read_previously_considered_positions) or pos_group[p1 + k_size] != pos_group[p2 - 1] ] 
                    # if len(not_prev_corrected_spans) > len(not_prev_corrected_spans3):
                    #     print(len(not_prev_corrected_spans), len(not_prev_corrected_spans3))
                    # not_prev_corrected_spans2 = [(m2,p2) for (m2,p2) in m1_curr_spans if not (p1 + k_size in read_prev_corrected_positions2 and p2 - 1 in read_prev_corrected_positions2[p1 + k_size]) ]
                    # print(len(not_prev_corrected_spans))
                    if not_prev_corrected_spans: # p1 + k_size not in read_previously_considered_positions:
                        # test_potential_speedup(not_prev_corrected_spans, minimizer_combinations_database, reads, r_id, m1, p1)
                        # sys.exit()
                        tmp_cnt, read_complexity_cnt = find_most_supported_span(r_id, m1, p1, not_prev_corrected_spans, minimizer_combinations_database, reads, all_intervals, k_size, tmp_cnt, read_complexity_cnt, quality_values_database, already_computed)

                print("{0} edlib invoked due to repeated anchors for this read.".format(read_complexity_cnt))

                print(tmp_cnt, "total computed editdist.")
                # sys.exit()
                eprint("Correcting read", r_id)
                # print(previously_corrected_regions[r_id])
                if previously_corrected_regions[r_id]: # add previously corrected regions in to the solver
                    all_intervals.extend(previously_corrected_regions[r_id])
                    del previously_corrected_regions[r_id]
                    # del stored_calculated_regions[r_id]
                # if r_id == 11:
                #     print(all_intervals)

                # for tmp_, inT_ in enumerate(all_intervals):
                #     print(tmp_, inT_)
                # SYS.EXIT()
                if not all_intervals:
                    eprint("Found nothing to correct")
                    corrected_seq = seq
                else:
                    all_intervals_sorted_by_finish = sorted(all_intervals, key = lambda x: x[1])
                    # print([(item[0],item[1]) for item in  all_intervals_sorted_by_finish])
                    # print(len(all_intervals_sorted_by_finish))
                    # sys.exit()
                    opt_indicies = solve_WIS(all_intervals_sorted_by_finish) # solve Weighted Interval Scheduling here to find set of best non overlapping intervals to correct over
                    # print(opt_indicies)
                    corrected_seq, other_reads_corrected_regions = correct_read(seq, opt_indicies[::-1], all_intervals_sorted_by_finish, k_size, args.outfolder, v_depth_ratio_threshold, max_seqs_to_spoa)
                    
                    for other_r_id, corrected_regions in other_reads_corrected_regions.items():
                        for corr_region in corrected_regions:
                            previously_corrected_regions[other_r_id].append(corr_region)

                corrected_reads[r_id] = (acc, corrected_seq, "+"*len(corrected_seq))

                print("@{0}\n{1}\n+\n{2}".format(acc, corrected_seq, "+"*len(corrected_seq) ))
                eprint("{0},{1}".format(r_id,corrected_seq))

                # eval_sim2(corrected_seq, seq, qual, tot_errors_before, tot_errors_after)

    eprint("tot_before:", tot_errors_before)
    eprint("tot_after:", sum(tot_errors_after.values()), tot_errors_after)
    eprint( len(corrected_reads))
    outfile = open(os.path.join(args.outfolder, "corrected_reads.fastq"), "w")

    for r_id, (acc, seq, qual) in corrected_reads.items():
        outfile.write("@{0}\n{1}\n+\n{2}\n".format(acc, seq, qual))
    outfile.close()



if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="De novo error correction of long-read transcriptome reads", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--version', action='version', version='%(prog)s 0.0.1')

    parser.add_argument('--fastq', type=str,  default=False, help='Path to input fastq file with reads')
    # parser.add_argument('--t', dest="nr_cores", type=int, default=8, help='Number of cores allocated for clustering')

    parser.add_argument('--k', type=int, default=8, help='Kmer size')
    parser.add_argument('--w', type=int, default=10, help='Window size')
    parser.add_argument('--xmin', type=int, default=14, help='Upper interval length')
    parser.add_argument('--xmax', type=int, default=80, help='Lower interval length')
    parser.add_argument('--T', type=float, default=0.2, help='Minimum fraction keeping substitution')
    parser.add_argument('--exact', action="store_true", help='Get exact solution for WIS for evary read (recalculating weights for each read (much slower but slightly more accuracy,\
                                                                 not to be used for clusters with over ~500 reads)')
    parser.add_argument('--max_seqs_to_spoa', type=int, default=200,  help='Maximum number of seqs to spoa')
    
    parser.add_argument('--exact_instance_limit', type=int, default=0,  help='Activates slower exact mode for instance smaller than this limit')
    # parser.add_argument('--w_equal_k_limit', type=int, default=0,  help='Sets w=k which is slower and more memory consuming but more accurate and useful for smalled clusters.')
    parser.add_argument('--set_w_dynamically', action="store_true", help='Set w = k + max(2*k, floor(cluster_size/1000)).')

    parser.add_argument('--compression', action="store_true", help='Use homopolymenr compressed reads. (Deprecated, because we will have fewer \
                                                                        minmimizer combinations to span regions in homopolymenr dense regions. Solution \
                                                                        could be to adjust upper interval legnth dynamically to guarantee a certain number of spanning intervals.')
    parser.add_argument('--outfolder', type=str,  default=None, help='A fasta file with transcripts that are shared between samples and have perfect illumina support.')
    # parser.add_argument('--pickled_subreads', type=str, help='Path to an already parsed subreads file in pickle format')
    # parser.set_defaults(which='main')
    args = parser.parse_args()


    if args.xmin < 2*args.k:
        args.xmin = 2*args.k
        eprint("xmin set to {0}".format(args.xmin))

    if len(sys.argv)==1:
        parser.print_help()
        sys.exit()
    if not args.fastq and not args.flnc and not  args.ccs:
        parser.print_help()
        sys.exit()




    if args.outfolder and not os.path.exists(args.outfolder):
        os.makedirs(args.outfolder)


    # edlib_module = 'edlib'
    parasail_module = 'parasail'
    # if edlib_module not in sys.modules:
    #     print('You have not imported the {0} module. Only performing clustering with mapping, i.e., no alignment.'.format(edlib_module))
    if parasail_module not in sys.modules:
        eprint('You have not imported the {0} module. Only performing clustering with mapping, i.e., no alignment!'.format(parasail_module))
        sys.exit(1)
    if 100 < args.w or args.w < args.k:
        eprint('Please specify a window of size larger or equal to k, and smaller than 100.')
        sys.exit(1)

    main(args)

