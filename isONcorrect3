#! /usr/bin/env python

from __future__ import print_function
import os,sys
import argparse

import errno
from time import time
import itertools

import math
import re
from collections import deque
from collections import defaultdict

import edlib

from modules import correct_seqs, create_augmented_reference, align, help_functions





def get_kmer_minimizers(seq, k_size, w_size):
    # kmers = [seq[i:i+k_size] for i in range(len(seq)-k_size) ]
    w = w_size - k_size
    window_kmers = deque([seq[i:i+k_size] for i in range(w +1)])
    curr_min = min(window_kmers)
    minimizers = [ (curr_min, list(window_kmers).index(curr_min)) ]

    for i in range(w+1,len(seq) - k_size):
        new_kmer = seq[i:i+k_size]
        # updateing window
        discarded_kmer = window_kmers.popleft()
        window_kmers.append(new_kmer)

        # we have discarded previous windows minimizer, look for new minimizer brute force
        if curr_min == discarded_kmer: 
            curr_min = min(window_kmers)
            minimizers.append( (curr_min, list(window_kmers).index(curr_min) + i - w ) )

        # Previous minimizer still in window, we only need to compare with the recently added kmer 
        elif new_kmer < curr_min:
            curr_min = new_kmer
            minimizers.append( (curr_min, i) )

    return minimizers

def get_kmer_maximizers(seq, k_size, w_size):
    # kmers = [seq[i:i+k_size] for i in range(len(seq)-k_size) ]
    w = w_size - k_size
    window_kmers = deque([seq[i:i+k_size] for i in range(w +1)])
    curr_min = max(window_kmers)
    minimizers = [ (curr_min, list(window_kmers).index(curr_min)) ]

    for i in range(w+1,len(seq) - k_size):
        new_kmer = seq[i:i+k_size]
        # updateing window
        discarded_kmer = window_kmers.popleft()
        window_kmers.append(new_kmer)

        # we have discarded previous windows minimizer, look for new minimizer brute force
        if curr_min == discarded_kmer: 
            curr_min = max(window_kmers)
            minimizers.append( (curr_min, list(window_kmers).index(curr_min) + i - w ) )

        # Previous minimizer still in window, we only need to compare with the recently added kmer 
        elif new_kmer > curr_min:
            curr_min = new_kmer
            minimizers.append( (curr_min, i) )

    return minimizers


def get_minimizers_and_positions_compressed(reads, w, k, hash_fcn):
    # 1. homopolymenr compress read and obtain minimizers
    M = {}
    for r_id in reads:
        (acc, seq, qual) = reads[r_id]

        seq_hpol_comp = ''.join(ch for ch, _ in itertools.groupby(seq))

        if hash_fcn == "lex":
            minimizers = get_kmer_minimizers(seq_hpol_comp, k, w)
        elif hash_fcn == "rev_lex":
            minimizers = get_kmer_maximizers(seq_hpol_comp, k, w)

        indices = [i for i, (n1,n2) in enumerate(zip(seq[:-1],seq[1:])) if n1 != n2] # indicies we want to take quality values from to get quality string of homopolymer compressed read 
        indices.append(len(seq) - 1)
        positions_in_non_compressed_sring = [(m, indices[p]) for m, p in minimizers ]
        M[r_id] = positions_in_non_compressed_sring

    return M


def get_minimizers_and_positions(reads, w, k, hash_fcn):
    # 1. homopolymenr compress read and obtain minimizers
    M = {}
    for r_id in reads:
        (acc, seq, qual) = reads[r_id]

        seq_hpol_comp = ''.join(ch for ch, _ in itertools.groupby(seq))

        if hash_fcn == "lex":
            minimizers = get_kmer_minimizers(seq, k, w)
        elif hash_fcn == "rev_lex":
            minimizers = get_kmer_maximizers(seq, k, w)

        M[r_id] = minimizers

    return M


# def get_minimizers_comb(reads, M, k, x):
#     reads_to_minimizer_combinations_database = {}
#     M2 = defaultdict(lambda: defaultdict(list))
#     for r_id in M:
#         # print(reads[r_id][1])
#         reads_to_M2[r_id] = [] # defaultdict(list)
#         minimizers = M[r_id]
#         for (m1,p1), (m2, p2) in  minimizers_comb_iterator(minimizers, k, x):
#             # reads_to_M2[r_id][(m1, m2)].append((p1,p2))
#             reads_to_M2[r_id].append( ((m1,p1), (m2,p2)) )
#             M2[(m1, m2)][r_id].append((p1,p2))
#     return reads_to_M2, M2

def get_minimizer_combinations_database(reads, M, k, x):
    M2 = defaultdict(lambda: defaultdict(list))
    for r_id in M:
        minimizers = M[r_id]
        for (m1,p1), m1_curr_spans in  minimizers_comb_iterator(minimizers, k, x):
            for (m2, p2) in m1_curr_spans:
                M2[m1][m2].append((r_id, p1, p2))
    return M2



def minimizers_comb_iterator(minimizers, k, x):
    # print("read")
    for i, (m1, p1) in enumerate(minimizers[:-1]):
        m1_curr_spans = []
        for j, (m2, p2) in enumerate(minimizers[i+1:]):
            if 2*k < p2 - p1 and p2 - p1 <= x:
                m1_curr_spans.append( (m2, p2) )
                # yield (m1,p1), (m2, p2) 
            elif p2 - p1 > x:
                break
        yield (m1, p1), m1_curr_spans


# def time_test(sorted_left_reads, sorted_right_reads, relevant_reads, r_id, k, x):
#     curr_min_r_pos = 0
#     for l_pos in sorted_left_reads:
#         tmp_add = 0
#         for j, r_pos in enumerate(sorted_right_reads[curr_min_r_pos:]):
#             if k < r_pos - l_pos <= x:
#                 relevant_reads[r_id].append((l_pos, r_pos))
#             elif l_pos > r_pos:
#                 tmp_add = j 
#                 continue
#             elif r_pos > l_pos + x:
#                 break
#         curr_min_r_pos += tmp_add   


# def get_relevant_reads(M2, m1, m2):
#     return M2[m1][m2]



def edlib_alignment(x, y, k):
    # if i == 100 and j % 1000 == 0:
    #     print("Edlib processed alignments: {0}".format(j+1))

    result = edlib.align(x,y, "NW", 'distance', k) # , task="path")
    ed = result["editDistance"]
    return ed

def get_best_correction(curr_best_seqs, k_size):
    r_seq = curr_best_seqs["curr_read"][0]
    # print(r_seq)
    # print(curr_best_seqs)
    # sys.exit()
    #### TMP
    #### TMP
    reads_path = open("/Users/kxs624/tmp/tmp_isoncorrect2/test/1/isoncorrect/reads_tmp.fa", "w")
    for acc, (seq, qual) in curr_best_seqs.items():
        reads_path.write(">{0}\n{1}\n".format(acc, seq))
    reads_path.close()
    spoa_ref, msa = create_augmented_reference.run_spoa(reads_path.name, "/dev/null", "/Users/kxs624/tmp/tmp_isoncorrect2/test/1/isoncorrect/spoa_tmp.fa", "spoa", "/dev/null")
    print()
    print(spoa_ref)
    #### TMP
    #### TMP

    partition = {"ref" : (0, spoa_ref, spoa_ref, 1)}
    # partition = {"ref" : (0, r_seq, r_seq, 1)}
    for q_id, (s_tmp, s_qual) in curr_best_seqs.items():
        # if s_tmp == r_seq:
        #     continue
        res = edlib.align(s_tmp, spoa_ref, task="path", mode="NW")
        # result_read_to_ref = edlib.align(seq, ref_tmp, task="path", mode="HW")
        # print(result_read_to_ref)
        cigar_string = res["cigar"]
        read_alignment, ref_alignment = help_functions.cigar_to_seq(cigar_string, s_tmp, spoa_ref)
        partition[q_id] = (res["editDistance"], ref_alignment, read_alignment, 1)
        # print(read_alignment, ref_alignment)
    alignment_matrix = correct_seqs.create_multialignment_matrix(partition)
    # print(alignment_matrix)
    # for r_tmp, aln_list in alignment_matrix.items():
    #     print(aln_list)
    nr_columns = len(alignment_matrix["ref"])
    PFM = [{"A": 0, "C": 0, "G": 0, "T": 0, "U" : 0, "-": 0} for j in range(nr_columns)]
    for r_tmp, aln_list in alignment_matrix.items():
        if r_tmp == "ref":
            continue
        for j, n in enumerate(aln_list):
            PFM[j][n] += 1
    # print(PFM)
    corr = []
    for d in PFM:
        n_max, count_max = max(d.items(), key = lambda x: x[1])
        if n_max == "-":
            c, n_max2 = max([(c,n) for n,c in d.items() if n != "-"])
        if c > 0.4*count_max:
            # print(c, count_max)
            n_max = n_max2
        corr.append(n_max)
    corr = "".join([n for n in corr if n != "-"])
    # print(corr)
    # sys.exit()
    return corr[k_size:-k_size]

def solve_WIS(all_intervals_sorted_by_finish):
    p = [None]
    v = [None] + [w for (start, stop, w, _) in all_intervals_sorted_by_finish]
    for j, (start, stop, w, _) in enumerate(all_intervals_sorted_by_finish):
        if j == 0:
            p.append(0)
            continue

        k = j - 1
        while k >= 0:
            if all_intervals_sorted_by_finish[k][1] <= start:
                break
            k -= 1
        p.append(max(0,k))

    for i, interval in enumerate(all_intervals_sorted_by_finish):
        print(i, p[i], interval[0], interval[1], interval[2])

    OPT = [0]
    for j in range(1, len(all_intervals_sorted_by_finish) +1):
        OPT.append( max(v[j] + OPT[ p[j] ], OPT[j-1] ) )
    # print(OPT, OPT[-1], all_intervals_sorted_by_finish)

    assert len(p) == len(all_intervals_sorted_by_finish) + 1 == len(v) == len(OPT)

    # Find solution
    def find_solution(j, opt_indicies): 
        if j == 0:
            return 
        elif v[j] + OPT[p[j]] > OPT[j-1]:
            # print(j)
            opt_indicies.append(j - 1) # we have shifted all indices forward by one so we neew to reduce to j -1 because of indexing in python works
            find_solution( p[j], opt_indicies ) 
        else:
            find_solution(j-1, opt_indicies) 
    opt_indicies = []
    find_solution(len(all_intervals_sorted_by_finish), opt_indicies)
    return opt_indicies

D = {chr(i) : min( 10**( - (ord(chr(i)) - 33)/10.0 ), 0.79433)  for i in range(128)}


def eval_sim(corr, seq, qual, tot_before, tot_after):
    true = "AGCAATCAGACTCAAGCATTAAAATACCCGCCCAGAATATGCCATCAAACTGCCTGGCCCGACAAATTAATGCTGAAATTGTTAACTGACCGGAGGTCGACGTACCGACACTAGGAGGAGCGGGACTGTAGGAAAACCACTATCCTTTTC"
    r1 = edlib.align(true, corr, task="path", mode="NW")
    ref_alignment1, read_alignment = help_functions.cigar_to_seq(r1["cigar"], true, corr)
    r2 = edlib.align(true, seq, task="path", mode="NW")
    ref_alignment2, read_orig_alignment = help_functions.cigar_to_seq(r2["cigar"], true, seq)
    tot_before += r2["editDistance"]
    tot_after += r1["editDistance"]
    print(ref_alignment1)
    print(read_alignment, r1["editDistance"], [p for p, (n1,n2) in enumerate(zip(read_alignment, ref_alignment1)) if n1 != n2 ])
    print(ref_alignment2)
    print(read_orig_alignment, r2["editDistance"])
    print(qual)
    return tot_before, tot_after


def find_most_supported_span(r_id, m1, p1, m1_curr_spans, minimizer_combinations_database, reads, all_intervals, k_size):

    # curr_m_pos = read_min_comb[0][0][1]
    # curr_m_pos2 = read_min_comb[0][1][1]
    acc, seq, qual = reads[r_id]
    curr_best_seqs = {}
    curr_best_seqs_to_id = {}
    for (m2,p2) in m1_curr_spans:
        relevant_reads = minimizer_combinations_database[m1][m2]
        seqs = {}
        if len(relevant_reads) >= max(3,len(curr_best_seqs)):
            ref_seq = seq[p1  : p2 + k_size]
            ref_qual = qual[p1 : p2 + k_size]
            p_error_ref =  sum([D[char_] for char_ in ref_qual])/len(ref_qual) 

            seqs["curr_read"] = (ref_seq, ref_qual)
            seqs_to_id = { ref_seq : [ ("curr_read",p1, p2)]}

            # print(p1, p2) #, relevant_reads)
            # print(m1,m2)
            # print(r_id, seq[p1:p2])
            for relevant_read_id, pos1, pos2 in relevant_reads:
                if r_id  == relevant_read_id:
                    continue
                read_seq = reads[relevant_read_id][1][pos1: pos2 + k_size]
                read_qual = reads[relevant_read_id][2][pos1: pos2 + k_size]
                # print(seq[p1+k_size : p2], read_seq, p1, p2, pos1, pos2, reads[relevant_read_id][1][pos1 : pos1 + k_size ], seq[p1:p1+k_size], reads[relevant_read_id][1][pos2 : pos2 +k_size],  seq[p2:p2+k_size])
                # print(edlib_alignment(ref_seq, read_seq, len(ref_seq)),  ref_seq, read_seq, (pos1, pos2), (p1,p2), relevant_read_id)
                p_error_sum_thresh = p_error_ref + sum([D[char_] for char_ in read_qual])/len(read_qual) + 0.1
                # print("p_error_sum", p_error_sum)
                if edlib_alignment(ref_seq, read_seq, p_error_sum_thresh*len(ref_seq)) > 0:   
                # if reads[relevant_read_id][1][pos1 : pos1 + k_size ] ==  seq[p1:p1+k_size] and reads[relevant_read_id][1][pos2 : pos2 +k_size] ==  seq[p2:p2+k_size]:
                    seqs[relevant_read_id] = (read_seq, read_qual)
                    if read_seq in seqs_to_id:
                        seqs_to_id[read_seq].append( (relevant_read_id, pos1 + k_size, pos2 ) )
                    else:
                        seqs_to_id[read_seq] = [(relevant_read_id, pos1 + k_size, pos2 )]

                    # print(relevant_read_id, read_seq)
            # print(p1,p2, len(seqs))


        if len(seqs) > len(curr_best_seqs):
            curr_best_seqs = seqs
            curr_best_seqs_to_id = seqs_to_id
            curr_best_stop = p2


    if len(curr_best_seqs) > 2:
        all_intervals.append( (p1 + k_size, curr_best_stop,  len(curr_best_seqs), curr_best_seqs) )
        # print("Best at window startpoint:", curr_m_pos)
        # print(len(curr_best_seqs),curr_best_seqs ) #,curr_best_seqs_to_id)
            # print()



def main(args):
    start = time()
    reads = { i : (acc, seq, qual) for i, (acc, (seq, qual)) in enumerate(help_functions.readfq(open(args.fastq, 'r')))}
    print("Correcting {0} reads.".format(len(reads)))
    start = time()
    corrected_reads = {}
    for k_size in range(args.k, args.k +1):
        # DBG, position_DBG = create_augmented_reference.kmer_counter(reads, k_size)
        # print("done createng DB")
        w = args.w
        T = args.T
        for hash_fcn in ["lex"]: # ["lex"]: #  add "rev_lex" # add four others
            if args.compression:
                minimizer_database  = get_minimizers_and_positions_compressed(reads, w, k_size, hash_fcn)
            else:
                minimizer_database  = get_minimizers_and_positions(reads, w, k_size, hash_fcn)

            # reads_to_M2, M2 = get_minimizers_comb(reads, minimizer_database, k_size, args.T)
            minimizer_combinations_database = get_minimizer_combinations_database(reads, minimizer_database, k_size, T)
            # print(minimizer_database)
            print("done creating minimizer combinations")

            # print( [ (xx, len(reads_to_M2[xx])) for xx in reads_to_M2 ])
            # sys.exit()
            corrected_reads = {}
            tot_before = 0
            tot_after = 0
            tot_corr = 0
            for r_id in sorted(reads, reverse=True):
                read_min_comb = [ ((m1,p1), m1_curr_spans) for (m1,p1), m1_curr_spans in  minimizers_comb_iterator(minimizer_database[r_id], k_size, T)]

                corr_pos = []
                (acc, seq, qual) = reads[r_id]
                # print("starting correcting:", seq)
                all_intervals = []
                for (m1,p1), m1_curr_spans in read_min_comb: 
                    find_most_supported_span(r_id, m1, p1, m1_curr_spans, minimizer_combinations_database, reads, all_intervals, k_size )

    

                
                print("Correcting read", r_id)
                # for tmp_, inT_ in enumerate(all_intervals):
                #     print(tmp_, inT_)
                # SYS.EXIT()
                if not all_intervals:
                    print("Found nothing to correct")
                    corrected_reads[r_id] = (acc, seq, qual)
                else:
                    all_intervals_sorted_by_finish = sorted(all_intervals, key = lambda x: x[1])
                    # print(all_intervals_sorted_by_finish)
                    opt_indicies = solve_WIS(all_intervals_sorted_by_finish) # solve Weighted Interval Scheduling here to find set of best non overlapping intervals to correct over
                    corr_seq = []
                    print(opt_indicies)
                    for j in opt_indicies:
                        # print(all_intervals_sorted_by_finish[j])
                        start, stop, weights, instance = all_intervals_sorted_by_finish[j]
                        print(start, stop, weights)
                        best_corr = get_best_correction(instance, k_size) # store all corrected regions within all reads in large container and keep track when correcting new read to not re-compute these regions
                        # print(seq[start: stop],  best_corr)
                        corr_seq.append((start,stop, best_corr))
                    corr_seq = corr_seq[::-1]
                    # print(corr_seq, len(seq))
                    tmp = [seq[0 : corr_seq[0][0]] ]
                    for cnt, (start_, stop_, seq_segment) in enumerate(corr_seq):
                        tmp.append(seq_segment)
                        # print(tmp)

                        if cnt == len(corr_seq) - 1:
                            tmp.append( seq[ stop_ : ] )
                        else:
                            print(cnt)
                            tmp.append( seq[ stop_ : corr_seq[cnt+1][0]] )
                                    
                    corr = "".join([s for s in tmp])
                    corrected_reads[r_id] = (acc, corr, "+"*len(corr))


                tot_before, tot_after = eval_sim(corr, seq, qual, tot_before, tot_after)

    print("tot_before:", tot_before)
    print("tot_after:", tot_after)
    outfile = open(os.path.join(args.outfolder, "corrected_reads2.fastq"), "w")
    print( len(corrected_reads))
    for r_id, (acc, seq, qual) in corrected_reads.items():
        outfile.write("@{0}\n{1}\n+\n{2}\n".format(acc, seq, qual))
    outfile.close()



if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="De novo clustering of long-read transcriptome reads", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--version', action='version', version='%(prog)s 0.0.2')

    parser.add_argument('--fastq', type=str,  default=False, help='Path to input fastq file with reads')
    # parser.add_argument('--t', dest="nr_cores", type=int, default=8, help='Number of cores allocated for clustering')

    parser.add_argument('--k', type=int, default=7, help='Kmer size')
    parser.add_argument('--w', type=int, default=7, help='Window size')
    parser.add_argument('--T', type=int, default=60, help='Upper interval length')
    parser.add_argument('--compression', type=bool, default=False, help='Use homopolymenr compressed reads. (Deprecated, because we will have fewer \
                                                                        minmimizer combinations to span regions in homopolymenr dense regions. Solution \
                                                                        could be to adjust upper interval legnth dynamically to guarantee a certain number of spanning intervals.')
    parser.add_argument('--outfolder', type=str,  default=None, help='A fasta file with transcripts that are shared between samples and have perfect illumina support.')
    # parser.add_argument('--pickled_subreads', type=str, help='Path to an already parsed subreads file in pickle format')
    # parser.set_defaults(which='main')
    args = parser.parse_args()




    if len(sys.argv)==1:
        parser.print_help()
        sys.exit()
    if not args.fastq and not args.flnc and not  args.ccs:
        parser.print_help()
        sys.exit()


    if args.outfolder and not os.path.exists(args.outfolder):
        os.makedirs(args.outfolder)


    # edlib_module = 'edlib'
    parasail_module = 'parasail'
    # if edlib_module not in sys.modules:
    #     print('You have not imported the {0} module. Only performing clustering with mapping, i.e., no alignment.'.format(edlib_module))
    if parasail_module not in sys.modules:
        print('You have not imported the {0} module. Only performing clustering with mapping, i.e., no alignment!'.format(parasail_module))
        sys.exit(1)
    if 100 < args.w or args.w < args.k:
        print('Please specify a window of size larger or equal to k, and smaller than 100.')
        sys.exit(1)

    main(args)

